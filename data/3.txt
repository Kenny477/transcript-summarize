 Okay. We are glad to have today. Judge had gone with assistant professor. We know about men, but yeah, I hope we hope that they're in few months. He will be associate professor in our department. And he finished his PhD in, and you gotta, gotta speeding 2014 for University of Illinois at Urbana-Champaign, which looks like here. I share with you my sort of Alma mater. I did my master's there. And the weather in the weather in Los Angeles, you shouldn't minus much better than my mushroom painted at night and day. He could do he using the same type of work as we have seen last week, which is machine learning but not do much imploded deep learning. And you recall that was the last time what less than two weeks ago? The last time you managed to my my assertion that this deep learning will never be. We can then we may not the, it's not like a normal software that we can verify and give guarantees 100 percent here and there will be always some, some possibility of mistake eukaryote because we cannot anticipate all the situation in the world that the software will encounter. So there is a question here. What is the theoretical foundation of this animal? Which is we created, but we don't understand how it works. So Professor Johnson, who is the time to understand the mathematical underpinning behind it. And I wouldn't say good luck. But he'll try to convince us that there is some understanding to be gained. So you see actually the title of the talk starts with understanding and backstage. And I just wanted to say that you see and has become a powerhouse in machine learning. And actually in some banking and stuff like this, a computer science department. Thanks to new professor, system office. So Chunxiang, we have improved our standing in the world considerably. Forman from 14, from from the rank of 14 to the rank of 11. So Chunxiang flow is used to, to give us about this deep learning. Deep learning. As Lindlar's, the deep learning machine will not do something. I'd expect the bus, but okay. Yeah, thank you. Thank you for that. Very nice introduction. And I'm very glad to be here to share with you some of my research in the foundation of the plenty. Title of my talk is understanding the improving and if our eating, adversarial robustness in deep learning, I will define deep learning, LFSR, Boston, yes, just seeing a few seconds. So let's work based on a lot of papers. I don't waste my students and the cadre test. So I'm directing the aesthetical machine learning live at UCLA. The focus of my group is machine learning. With several pillows under the big umbrella of machine learning, including foundations of the plenty. Non-convex on my suspicion, reinforcement learning, high-dimensional statics and their applications such as academic model. So if you are interested in what we are doing in my group, you can go to the lab website, which I'll show on the bottom of the slide. You can also go to my homepage and you can easily find my group website. So for some of you who are not familiar with machine learning. So machine learning is the study of computer algorithms that can then, from the experience and that can benefit from. Having a lot of beta. Euthyphro's, the impaction. Oh, I froze. Slide. Is only a lie. You like how communities of speech. Okay. Let me stop sharing and do that again. Sorry about that. Okay. How about now calcium I froze picture frozen speech rules. So that's okay. Yeah. Okay, cool. Yeah, so as I side of machine learning is the study of algorithms that can improve itself. And this is our typical machine learning pipeline. So it takes bait housing, put it about video data into some algorithms. And then the algorithm will be with some models which we can use for future prediction and decision-making. So there are many different paradigms in Machine Learning. For example, supervised learning. Unsupervised learning include the classification, the crashing, and unsupervised learning, which is mostly driven by data. Examples of unsupervised learning include the catherine dimension reduction and many other more periphery data analysis. And another paradigm is called a reinforcement learning. These days, it's a very popular because it's a cheap, a lot of success in different applications such as Africa, which is a computer program for the coal game which can beat the world champion in Colgate and defend the machine learning paradigm. Hello. And you question, if somebody wants to ask a question, raise your hand. The galleries. Just something you are taking over as well. Okay, Cool. Yeah, I saw machine learning can be many machine learning or practice. It can be boiled down to my featuring problems. For example, in supervised learning, we minimize some loss function between the ground truth label and the predicted label for unsupervised learning. Because we don't have label. Why don't we do something such as maximum likelihood estimation and reinforce the blending? There is a Markov decision process behind this whole process. And we can minimize and Obama era of the Markov decision process, which is another optimization problems. So to sum up four different tautomerization up a different machine learning problems. We can often formulated as an optimization problem where f is some object function anesthetized and a prime it opportunism object function. And the machine learning there is a very, very successful branch, which is the cauda equina. So deep learning is machine learning, which use neural networks. So for instance, this IF function can contains a big model which is a different hand up on your network. Let's say the infection. We want the neural network, which contain 50 million parameters and rest. An IPP, which can even 23 million Prime Cuts and resonate 152 on the AlexNet, which contain more than 60 million parameters. And the VGG 16 and put a trans woman. So these models are much, much bigger. B, contain more than 100 million parameters. And more recently, GPT-3, which is our model for natural language understanding and gyration. It contains more than 175 billion. Book size. Didn't change the reason shut them out. I'll change. Oh, sorry. I was not aware of that, so let me do it. Hi, everyone. Now you can see my uncle works. Okay, okay, cool. Yeah. So I just showed that this animation, which gives you a rough idea about different than you and I works with different sizes. So the most recent work, which is very successful, it's called a GPT-3, which is a neuronetwork spoil your natural language generation understanding for it contains more than 175 billion parameters. Okay? So as you can see, the planning is like an important branch of machine learning. If you use your books as the underlying model. Because we are lying you and I will often contains many, many parameters. So deep learning often boils down to an automatic thing problem, in particular, non-convex what Nassim problems, which has a optimizing some function of a large number of parameters of a neural network. And therefore it is computationally very expensive. And also it is that they had to Monday. So in order to share and you and I worked, we need a lot. And this causes a lot of both computational and aesthetical chime in for peatlands. And because of that, we need to on the one hand, we need to adjust the child's airway. Try different models. We, we tune the parameters of different deep learning. I always try to get a good performance. But on the other hand, we also want to have our Python standing out the theoretical foundation of the plan. Okay, so here's another picture which shows here the rights of the planning for about 10 years or younger years ago. If you are using chiral models, which is non deep-learning to do image recognition, you can achieve about like a 28% misclassification error. Bound is the image of DataSite image an active site. And in 2012, that year, there's a very famous and you and I walk was proposed or the AlexNet. And it contains eight layers. Is not a very deeply when I want, but I'm bad hand it layers these already considered as a deep model. So they use these eight layer neural network to produce misclassification error on this image at night, data starts from 25 or 28 to 16 percent, almost 110% improvement. And in 2014, there are two books. One is from Google, the other is from University of Oxford. So they both developed some new network, which are much deeper. For example, on a Google night. Later, they changed the name to be inception. So it contains 22 layers. You can achieve a misclassification error of 6.7%. The other group, they propose neural network, VGG net, which is also very famous neural network. It contains 19 layers and it can achieve 70, can achieve 2.3rd% misclassification out. So they almost are just reduced. The misclassification arrow from 10 percent to less than 10 percent. And in the year 2015, there is a very fair forever when I walk was proposed or the residual or reselect listening and I will create a very, very deep it contains 150, two layers. Even nowadays, we still consider a new network which can contain more than 100 alleles as a very deep noon hour. And by using this new network, they are able to deduce the misclassification error to 3.5%. And this arrow rate actually is already smaller than the recognition arrow achieved by humans. So in other words, if you ask the human to do image classification on this, the image of the image going at a site. The mismatch between arrow actually is higher than this 3.5%, which means on this particular immediate data site. And for this particular image recognition task. Deep learning about five or six years ago already beat the human. Humans. For example, recognition, skill. And for the Ryzen at the AMI, give you a picture about the residual network, re-check. This one, which contains more than 100 layers and hardship less than Forbes and this phishing URL on the ImageNet dataset. So little illustration of a residual night. So you can compare the residual networks. Now, the feedforward neural network for the bottom line is the standard feedforward neural network. So from the left to the right, it isn't the input and output. And you can see there are many rectangles in the graph of a feedforward neural network which correspond to different convolutional layers with different size of a few with her. And the main difference between vestigial network handle a plane that will, this is so-called a residual connection. Okay? So for every two layers, you join a skip connection from the input of the first layer to the output. Secondly, so for every Twitter here at New Journalism and skip action. And the intuition behind it is the stdio connection is, suppose you want to map x to be to some, for example, f of x. You want to approximate this function, f of x. So instead of directly approximate f of x, you are going to approximate f by x plus x. So in this case it is apa instead of isn't a function actually it becomes receipt of IP. Because under CEO of Nike is often like a lower order of magnitude bender finding value in itself. So by approximating like a law that and it will make the noon hour training easy and also much more stable. So that's the idea behind it is residual connection. And my researchers are trying to understand deep learning, as I mentioned before. The reason is, as you can see, deep learning is a very like the time and energy consuming machine learning paradigm. If we don't have a good outstanding, we may spend a lot of time and money on it, but we may not gain a lot from the tunnel under this environment. So unlike a traditional machine learning and AI, the theory behind deep-learning is still in its early stage. So there are some preliminary results. But he's another one standing. And we want theory to understand the size and the mysteries of deep learning. But basically we want to know why and how and when the plenty can be successful. And then we want to use this theory to guide the design of a next-generation deep learning architectures or even new paradigm. So if you feel the current prices of the plenty as like alchemy, okay, because when you try different, I wasn't models. You observe the outcome is successful or you think PDE and then you'll go back and forth to, to tune it. The theory of the planning will convert this out-compete to be regardless science, chemistry. And if we have a good understanding about deep-learning, then we will have a science out of people and instead of just some empirical success on the plane. All right, so in a lifestyle, but his talk, I will focus on adversarial robustness, the plenty. Because this is a wall, the central actions people are trying to answer in deep-learning. So in order to introduce robustness than me first tells you a very important concept. The other threads our example. So suppose you have an image or image of a pig and you have a new network which can achieve 91 bus recognition accuracy. And then you add some perturbation to this image of pig. And the perturbation is very, very small scale. And by adding this perturbation to the original image or a pig, you got a new image. And by human eyes, actually, if you look at the listing page on the left-hand side and this new image on the right hand side, you cannot tell a different location. These two images by, I don't think you can identify any notable difference. However, for the same neural network, which used to classify this image as a label of pig. It will misclassify the newly generated image to be a Ireland, which is not, which is not even an animal. And this is actually a very severe misclassification. And keep on making a joke about this example. They say all machine learning handmade, pick five. But, uh, you, you may think that this is a funny case in deep learning, but there's another example which is actually a ratio. It's a lot like a security concerns. So these examples, this example happens in the siting of self driving. You know, these days people are talking about Self-driving cars. You don't need to steer the wheel. You just stay there and a car can drive by itself, which is nice if required. Make sure the states That's less cube. And one of the important component in self jogging is an image recognition because you need to use some radars on camera to capture the different objects on the road. One of the object. Let's try with the sun. Do you need to know when to stop? Well, what's the current, the chart equal to light if it's a green light or red light. But so you need to stick out a lot of things. And in this sum, all the cameras after self-driving car, it takes, there's a stop sign of the input. And someone they may, for example, manipulate and these stops and by, for example, ID sticker on a stop sign or do something very, very minor. And I really liked the new network will miss class. Families a stop sign to be a yields. And these are very different traffic signs or stop sign the beak on you to fully stocked by the four yields and the child, but the vehicle does not have to fully stop. You can stop or you can go ahead, add to the discretion of the Java byte. So because of this misrecognition by this will cause potentially some, for example, accident or even cause human life, not only economic loss, but human-like loss. So this example tells you that it's very important to make sure that the plenty model is robust to this kind of a perturbation. Or we quite adversarial attack. And there are many, many changes it, or ensuring the robustness of the planning models. So it isn't going to focus on two of these trainings. The first charging is understanding the impact of your network actress on robustness. So in other words, for different kind of neural network, we want to figure out the influence or impact of the architecture of your network robustness. But secondly, training is how to evaluate our models. But before you eat the problem model, you want to have a magnet. So if our Asian of a model, in terms of its robustness, instead of just a, you'll come up with some model. Yup, are excited, you deploy model, then you may make some serious mistake that will cause a lot of economic or human life lost. I'll hit so I got some hatching. What's the difference between deep plenty and a traditional machine learning? So first of all, I would say be plenty is just a branch of machine learning. And the traditional machine learning is more on the chyle models, such as linear models. Kanamycin kind of isolate non-linear, but it's still considered as like a random feature miking or by some linear models. So we still consider it as a channel models. What deep-learning, I think that a key feature is that you can use a much, much deeper models such as and refigure night I just introduced. It contains more than 100 layers. So that's a key difference. So, so the key difference between traditional machine learning and deep learning, these are the model we are using. We are using deep models in D, plenty of France is shallow models in traditional machine. All right, so in order to make deep-learning model more robust, so instead of using standard training on left-hand side, we minimize some loss function. This loss function is defined on each chain data point x i, y i. So here theta of XYZ and the predicted label of stuff, um, let's see some new network which takes as input and output are some particular label. And then here y is the ground truth label. So you can think of this as an image and a y label of the image. And L is some loss function. You want to minimize the loss, the DA, whether you want to maximize the recognition accuracy, recognition performance on the chin downside. Have a list kind of standard training is sensitive to the other 500 samples. So in other words, it's sensitive to some other error perturbation of training data. If you modify the training data in a way similar to pigs example or to the stop sign example, then you may end up with a machine learning model that are not as good as, as robust as the model which are the product. Like a very clean environment. And in order to overcome these problems, people propose to do a depth-first search. So when you train a model, you already added some perturbation there. So we modify this standard training two-year chaining. So instead of minimizing the standard loss function, you are minimized some worst-case loss function. So for each training data X i, when you define a loss, you already defined the worst possible loss that is incurred when your particular with the original clean image or clean data. And your goal is to minimize this worst-case loss function means that Alpha Lisa, that's the case, our loss function. So the left-hand side, it's more like a minimized on average, kids are bicyclist loss function. And right hand side there is no minimizer, the worst-case loss function. So it is formulated as a min-max pragmatic problems and you can understand it as like a game. So the facility, they want to maximize loss. And we build up different models. We want to minimize this loss. So it's like a min-max game. And by doing that, you're actually, we observe that we are indeed improving robust things, alphabets, training, neural networks. Now, let me give you a formal definition of the facility example. So suppose you have x and its ground truth label is y. And the x here, x-height price defined as an example of the k from x. If, for example, x pride belongs to us, small neighborhood of x. So it must be a small perturbation is a far, far away Vmax, then we wouldn't consider the example x. And the second that we need, the predicted label is different from the grandchildren of x. So this is the definition of sample. Now, oh, they observe that if that using wider models, so in other words ie by using widen your networks, then it'll generally enjoy biter at FSL robustness for this observation. What's needed in this paper by emoji at all in Chinese. So they better they propose a hypothesis. So in order to separating other facility examples, we need to more complex decision boundaries and the larger model complexity. So why they made this happen? Think about at this point, some positive you have are these blue dot and also bringing thoughts. But each of them corresponds to an example corresponding to a data point. And the way you do at the facilitation will do scandal attorney, you are essentially just drawing a line that it can saturate. This blue dots and green dots. By the way, are two other facilities because of maximization problem defined inside because of this maximizing profit and the findings that you've been using this Manhattan distance or evening on, then the decision boundary will be more curved, okay? Will be if our placement straight line, it would be this very complicated non-linear decision boundary. And you hope it is non-linear decision boundary will not cross each company's squares. Centered at each change in quantity. Because the Ailey being a norm in two-dimensional space corresponding to a square. Okay? And then you want to not only correctly classify each opponent, but also correctly classified there or even the perturbation. So in order to achieve this goal, you need a decision boundary. I got a ROM and into squares instead of going through these squares. Okay? And because I'm noticing boundaries and more complicated, of course you need a model to have enough capacity to, to generate, listen, very non-linear decision boundary. Okay? But at least hypotheses are actually, as an adult, be carefully examine. Is it just a hypothesis? They neither mathematically or equally firefighting hypotheses systematically. So in one of our experiments are so ray parallel experiments on this image. Settler genocides. And we're using flourishing of residual and that color. Why did we see you and I? Because in wider with the unit vector r parameter, which controls the width of each layer. So we can live with that parameter to try different widths. So here we tried a new and I work with a width equal to 1, 3, 5, and 10. Okay? And then you can see that when we increase the width of the book, the natural accuracy, which is accuracy, we solid attack with other atoms or attack actually increase when we increase the width. But the loo robust accuracy first increase and then decrease. So you can see that the robustness actually does not monotonically increase as we increase the waist up and even at work. But this is a little bit the contract decreed to the hypothesis that in order to achieve better robustness, you need a wider network. So according to this experiment, this is not always the case. So it looks like the differences are trade-offs. And in order to mathematically describe these trade-offs, we divide the sample, the training sample into seed sites. The first sight it's harder. Robust example, which are those examples even after Beijing has still be correctly classified. And then your network. And then a secondary site is coded correctly. Hadoop and sample, which are those that are correctly classified. Cell cleave sample can be correctly classified. And the last one is called a stable example. Stable exam o means that predict the label of the perturbed data point is the same as the predictor label as the original clean examples. You can see that that actually the Craig, if you use a circle to withstand the correctly classified the humble, and then use another circle, this circle to represent the stable example than the robust the example is nothing but the intersection of these two circles. So you can see that the little robustly fumble actually is both correctly classified example and stable example. For this is a very important observation, although mathematically is not that difficult to, to figure out what makes something. We pan out in our recent work. And then we're going to just afraid that robustness and accuracy and Mr. BB, which corresponds to the ratio of the size of each of the site divided by the total number of examples. So by doing that, we are able to, we just plot the training curve of neural networks against the epochs. Epoch. This, when you train your network, you will repeatedly use the training side and each epoch responded to entire pan-Slav latrine datasets. And the y-axis is the robust accuracy which correspond to the size of this intersection divided by the total number of samples. And the natural accuracy which corresponded to light up blue circle divided by the total sample size. And also the stability which correspond to the size of this stable sample size divided by the total number of somehow. So you can see that under robust accuracy. The first one you try different the waste when you increase the width Avenue and I work, you can see that that robot as accurately actually go up and then go down by the four natural electricity. When you increase the weight, actually is a monotonically increasing. And the first ABD actually, when you increase the waste, it becomes less and less stable. Okay? So in other words, we can see that the waste improves natural inputs to this one. Button may not help model robustness. Because when you're inquisitive, not nice, It's really increase. And because of the diets, we, we want to come up with some analysis to understand why this is case. So we formulated the adult training framework has sort of following one where we minimize natural laws, punishing, regularized by some robustness feature. And this is nothing but adjust the triangle inequality has originally we want to minimize, we want to minimize this worst-case loss function. But now we just subtract standard or loss and then added the back. For this business. Something we often use when we derived the bone. Then we have these two parts, two terms. The first one is the standard training loss, and a second one is the difference between worst-case loss and understanding and love. And quieter as that baby on my graph. And we can evaluate this robust arrived relaxation using some surrogate quantity here. So we use the gradient norm, which quits on big, big because of the difference of two functions can be approximated by, by, by the norm of the gradient or by the derivative. So we're using this gradient norm to give right, robust things. And you can see that when we increase the weight something you and I would miss robustly regressing it and actually it'll decrease. So in other words, when we increase of waves, the last stability we will have for linear network. And here are some additional insights. So we just conducted more experiments by using different regressing parameters. Lambda is the regression parameters that encourage a robust and is on the stability. And in our work. And we tried to append the way something in our work. And we increased the regression parameters from six to 18. We gradually increase it. And we calculate the robots I, Christy and the deposit and other natural accuracy and the stability. And we report this table, as you can see. When we increase the waist up with you when I work alone, accuracy. What if we use, using larger and larger regressing primate and then actually we can make it a real-life accuracy, monotonically increasing. And for natural accuracy, as you can see, by using larger regression parameters, actually you will have a worse nitride. You can just read through numbers. This column, this column, when you increase the parameter, natural accuracy will decrease. And the four stability, it's an option. Each week when you increase the parameter, actually they will increase. That's they'd be. Under that robust accuracy can be used as a trade-off between natural to say Yeah Honest Abe, even though if the next target is decreasing and the stability is increasing, then there must exist some next tweet points such that robust accuracy is maximized. And this is exactly the case that is reflected in this table. So for waste one knew and I work. When we chosen the regression parameter Lambda equal to nine, we have the highest Roman sacrifices. And four with five b c units. If we choose regressing primary equal to 15, we can obtain the highest robustness. And similarly for week 10, I work. If we choose regression parameter to be 18, then we can achieve the highest, like lightening products. The list of suggested that in order to unleash the power of white on your network, we need to impose stronger and stronger regularization to encourage you. That's the beauty of them. And a similar thing had the opposite. Example 8, we use this idea and we share why tomatoes? Oh, let's step our tongue. You can see that by doing that, all my snare beat mistake a lot. Nicer by a big margin on the different attack. So here it is, PDT time, the C and W. So these are the different attack myself. So you can understand them using different a nice little January to the perturbation. That perturbation to pick an image or a perturbation to the stop sign. Okay. Let me see. What's it. Do this layers, okay? So for each layers, so think about like image recognition tasks. Each other, this mayor, they are doing image feature extraction. Because when you do convolution, it's like, uh, using some filters to check the features at different locations on body image. And you are doing this betrayed extraction layer by layer that will enable you to check that you tried a different level. So some of the tried for a lower level, some of which are at a higher level. So that's the function of different layers in your network. Okay? So the types correspond to the number of layers, the ways that they correspond to how many filters you are using each layer. Because at each filter will generate a channel. If you have a money field has grave monument channels. So number of channels will gives you. When we thought that an x prime means you're experimenting with kids occupation. Yeah. I didn't notice those questions, but they are all great questions. Okay. And due to time limit, probably I should just directly jump to the conclusion. I don't have time to tell you how to refer you to the robustness of our models. Maybe I can give you some quick overview. So improprieties, there are different types of attacks, such as a light box, white box attack. An attacker may know internal structure of the neural network. So they have a lot of information about the model you're using. And then a black-box attack means they don't know the internal information, nothing in our work or they can. But they know the logits and ultimately offering your network. And a Hollywood agonies. The attack the know anything about the network itself except the predicted label. So the Hollywood had actually is at most a particle one because in practice when you build a new model, you don't have to disclose leaks. Model of public with this publisher is model, right? You don't need to tell anyone else about an economic mission. But when you use it or when you give it to other people to use it, then they still know what to predict the labels for a holiday. But most of the practical citing and a white box attack is actually insulin. Most difficult setting because the attacker knows the most information, almost all information about hormonal UIUC. And for different attack myself, we want in order to refer you to the robustness of the model, then we want to use different methods to attack them on us. And because of how labor type is white box that attack, the more difficult it is hardly ever tidies. It actually starts the white-box. The tag actually isn't a single instance type. And another Hollywood hierarchy is the most challenging case because the attacker doesn't, doesn't know a lot about the model itself. So we need some information myself too. If our eight or no button that is often models. And in order to do that, we build a leaderboard. And the leader what is called an ADD, ADD the bone and you can find it is neither both groups did have lab page. And the goal of it is neither one is to provide a platform such that whenever you come up with a model which you believe it isn't robust than you submitted its model to our leaderboard, to the GitHub page. And we can help you to evaluate your model by using different attack myself included whitebox attack, black-box attack, hard labor type, because we implemented a different attack Ni3Al, including our own tokenizer. And this ADB, the ACT score. We're proposing our own paper. And we put a range of different robust models that are published in the literature on some of them, they are just the UE post the archive I haven't actually published, but you have submitted those models to our. So if our website and we will eukaryotes and your model and a compile our model with other models. And we will generate a ranking is replicate based on the ADB the score. The higher the score, the more robust the ammonoids. So this is like a screenshot of our leaderboard and perhaps happy years ago and now there are a lot more mono to be submitted to the board at the ranking, I'd be updated. And it could be very different from the ranking I showed you here. So in other words, in order to define the model, we, we put together a team and we build at least leaderboard and then go up this leaderboard is to provide a pace where you can compile our model. With other models. More attractive when. And we believe ADB, the score ether, uh, one of the machines that can reflect the true robustness of the model, the other, by something I didn't show is that there are some other metrics by using to you varied and a robust model. But it turns out that many checks are not able to reflect and a true robustness. So let me give you some force and robots, for example, using that match-up, it's saying that your model is a robot, but it's not robust. In fact, I, and so what is a logic module, me, because I'm in many new networks and the output layer is cascading layer. So you need to use softmax function to map the internal to some probability distribution and each bit of the probability the shooting is required. So that logic outputs. All right, so to sum up, so in this talk by giving you a very detailed instruction to show that wider network bandwidth and a robust regression effect. And therefore, we need to increase the robots by graphing parameter to improve performance. Under the second part is about how to if our model and detect windows and fourthly, robot models. And I didn't have time to go into details, but I show you that we have a leaderboard handy. I go to this website to try it, if you will, in the future, have any model that you think are robust. You can use this leader wanted to, to test for touch five-year period, really robust argument just as like a fourth, robust things. All right. So with that, I'd like to thank you for your time and I'm happy to take questions. Do you have any question? Things that will get you to the front? So sweetly. Anybody know someone, someone raised a hand. I think just a really good question. What is ADB dissent freedom. Okay? This is like a hinge. So this AD BD is harder, average a decision boundary distance. Because I think about that when you do hard labor attack, you have, for example, to cast a public forum posts. And this isn't a decision boundary. And if we want to attack the examples, we basically want to perturb, for example, this green. I'll start to perturb it to make it across the decision boundary. Because once you cross the boundary, it will be misclassified. And what's illustrated the past, we need to what is the shortest distance? We need to perturb this example to make it be misconduct. And this is nothing but adjust the distance to this same model. And the way I use the average of the sample average distance to like a thin boundary. We average all those examples. And this gives you a score corner, ADB. And this can be used to measure the robustness of the model. Cooler. Hello, I wanted to sneak in a individual. Who's talking. Yeah. Okay. Yeah, I can see it. Yeah. So can you explain what an attack is and what the residual network is? Sure. So okay, so the attack I'm talking about in this talk is the part that task. So in the test-time, in other words, when you deploy your model and then there comes a new images or whichever. Let's say this Guinea pig attack means we want to modify this image. There are some attackers. They want to modify this image to make your model make Mischa's patient. And if your model make an image classification, we call this a tag less than successful attack. Even up to the example sui, be modify our image about the 4Ps model has still me. I create a prediction. Then it means that the attack rate is not as successful. So this is the definition of attack. And therefore last night, economy is still nice. And so the key logical indexing that is called a residual connection, which has been a skip connection. You just jot and Binaural link from the input of the first layer to the output of the second layer. And because of this, the skip connections, I believe it will make a changing our beliefs that people know I work more stable. So that's the rough idea behind it, is reciprocal networks, the formal definition, a mathematical definition. But let's proceed with that. Work will be a little bit more complicated, but you can still write it down. You can define them because me, for either define a recursive and payback residue night or you can go to this, I think in this block. But this other, he wrote a lot of articles about different machine learning models, including dependent deep learning models. So actually this bigger, I just pulled from this prompts when you go to this prompt and they provide a more detailed introduction to the architecture of the CPU that okay, okay. No problem. Cushion. Between, between machine learning and human beings, I think is something we'd like to make some sense, higher sense of it. Okay. Which is simply see the peak. We see eyes, we identify. These are, these are ears. Yay. You can see why in some way, why? Why a machine can make a mistake. Because the machine extract some features but it doesn't know and doesn't understand the feature maps that make it so you, if you see a pig, because it has these big ears, in some way, I can look like the rings airplane. So this is the third airplane with a we don't know what what people say, What is missing right now. But somehow combination of high level understanding and what we are doing here is probably what our heads are doing, the most wounding mentally level. And then we tried to make sense not only that, these are years of stumbling. Psb. We try to, this is another, We don't have it a talk by dispersal narrow about men, the blue ink also, when we analyze things and cause an effect, we actually understand the world only by cause and effect. Human being, just by human beings don't make just now very good correlation as much as doing cause and effect. And right now, we don't have no one else, we don't have cause and effect. So this is there, there is a lot of research share to do that. Probably either level, but even at this low level, we have a huge successes. So the huge successes are actually replacing, replacing low-level work, say will. In this would be more new lifetime that a lot of work and choose what to cut routine and we will not get people machine will be able to do that so much that this is extremely successful. And I already mentioned that evolution. But there is way to go until it will be, until intuitive commonsense. Okay, any item we don't have time running out of time, so you can piece able to stop the recording. And everybody, we are now I'm 30.
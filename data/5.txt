 So we have to do our due date. Sherry NAFLD Burke. Carrie has graduated from UCLA, maybe some masters in 96. But in a way he didn't leave UCLA or 95. It didn't leave UCLA. He has been with us, you know, teaching here at, teaching there always helping consider and considered the best, the best lecture. And in our, in our department. Is it despite his name, goes ahead of him. Everybody knows that. Hello bang. But in the meantime, between there, between students at UCLA, he worked in semantic, which year he designed who's responsible for that knocked on their antivirus. And then the large scale as opposed to seeing systems in Google X. And then move to leave. Well, he worked or level 5? Level five is the highest level. And perhaps later they didn't explain to us why, if it's a game of data, why Tesla will not win? So, okay. Now here is an image. When you do welding on jobs so you can retire. So is contemplating that if he loves TJ do much, you may see full-time teaching goals stark dark, and as somebody asked, she will be teaching in the winter. So you will be teaching CS, so they do. So you have an opportunity, I will not competitive Duarte us, but we have an opportunity to take the course with him, so I'm going to back. Okay. So Carey, There you go to talk about broadly what we got last time, which is the same like I pursue. We're going to talk about self-driving cars today. And after we're done, everybody's going to know exactly how to build a self-driving cars that are pretty exciting, right? So let me share my screen. Thank you for the nice production. And let us go here. So can everybody see my screen, please? Yes, 100 vehicles. Okay. Let's just start. Can you see that? It's presented is okay. Perfect. All right. So the title of my talk, everybody is autonomous vehicles 101. And the subtitle is building your own self-driving car for fun and profit. But of course, we all know that there's no profit and building a self-driving car, it's extremely expensive. And it just costs hundreds of millions of dollars a year for the hardware and the software. The GPUs, GPUs are hardware used to train neural networks are extremely expensive to train, to train these systems and build a prominent at least yet. But hopefully you'll enjoy the top anyway. And here's a great quote that I like 1 second here. I'm just glad there isn't a self-driving car built by Microsoft because everyday there be multiple crashes. Okay. Good. Okay. All right. Obviously people are not laughing at this or if they are maybe that Rob needed. Okay. So let's let's keep going. So you may have heard this, you may have not. There are multiple levels of autonomy that people talk about when they talk about self-driving cars. So level 0 would be a car, an older car. For instance, you might be driving that has no cruise control or anything else. Literally it's you or your hands on the wheel, on the accelerator, the break, your feet. And that's a level 0 autonomy. Level one autonomy might be basic assistance like cruise control, where you basically set the car 50 miles per hour and it just drives and you know what? It'll drive into a wall or another car if there happens to be one there because it's not really intelligent. Now, level to autonomy is, now we're starting to talk about like let's say the Tesla's of the world baby might be a level 3 a Tesla. And basically the carpet accelerate, steer on its own. It can it can potentially break if there is a car slowing down, but it has to be monitored by the driver. In other words, a driver has to pay attention because a car is not great, the autonomy system is not great and it might make lots of mistakes. Level three is a situation where the driver doesn't have to even keep their eyes on the road. In most circumstances are many circumstances, it can literally read a book or something and it's legal to do so. And there are really no cars like this on the road today that are in mass distribution. So this would be starting where the way most of the world which are in this level, or maybe a little bit higher. Right now, the Teslas are aspiring to be here, but if you take your hand off a Tesla, we've seen what happened in the news. Sometimes, you know, level four is where most autonomous vehicle companies want to hit. And that is basically fully autonomous driving under certain circumstances like it's sunny out. No, there's no rain, there's no snow, it's not the roads aren't slick. Basically search where we know the vehicle. And level five is the economists, which means you can take that car and you can put it in a sandstorm, a windstorm. It can be raining out. There, can be still in the ground. A cat can run in front of the car. An alien can basically try to touchdown on the, from the outer space onto the freeway and the Carlsberg around it and figure that out. This is the holy grail of autonomy and nobody is really trying for this. So all the companies are trying for level 4. All right, so without further ado, so you're going to hear me use the word ego in this talk. And ego is the name of the self-driving car. So that's the way if you are in the industry, you say, Oh, how is, how is ego doing? Did it did it slam on the brakes too hard? You know, Diego navigate through the intersection properly. And so the ego is just a term you'll hear me use and I don't want you to make sure that you understand that. So there we go. Now you know the terminology. Okay, so let's talk about an overview of autonomy overall. So you can understand like what are all the pieces that really make an autonomous vehicle system. So first of all, there's mapping. So, you know, in China's vehicle or most autonomous vehicles, with the exception of maybe Tesla a little bit, need lots of maps with various information, those maps, and we'll talk about this in order to properly navigate around, make sure they stopped when the desktop and interception and so on. And we'll talk about this. So mapping is a key input to an autonomous vehicle system and these have to be built ahead of time. These maps, there's route planning. Route planning, just like Google Maps, where you do navigation, turn-by-turn navigation, times, vehicles need those books. They need to know where to go. Perception. So perception is all about perceiving the world around you or world around you go. And so perceiving other objects like a cat, a pedestrian and vehicle up, perceiving things like a stop sign or a stoplight or a sign that says roadwork ahead or something. And also not just perceiving that there is a car, but tracking where that car is over time, say, Oh, it's here now and it's going to be there a second later and so on. Okay, so perception is about knowing where things are identifying them and then identifying where they're moving over time, tracking them over time. Localization is all about figuring our ego is. So for instance, think about like GPS, that would be a basic way of doing this. But how do you know where the self-driving cars? Because the self-driving car doesn't know where it is and it doesn't know where it is with respect to the map. And it's going to have problems and you have to have very, very precise localization for self-driving cars. It knows exactly where it is because if it's a foot too far to the left or right, my Google Drive over curve for campbell prediction. So predictions all about predicting where other entities will be in the future. Where's that pedestrian going to be? Are they going to cross the street or they're going to stay looking at their watch. Are they going to spin around and talk to their friend? Bicyclists going to go and turn into traffic. Are they going to try and make a left turn in front of you because they're putting their hand up. Yeah. There's lots of things you have to do in terms of predicting where other entities will be in the future. Now, if we have predictions and we know where we are relative to the map, and we know where all the other objects are. We then want to fan or behaviors. We want to figure out where should our vehicle go, where are the constraints for the vehicle shouldn't go? And work my word, where do we want it to go? And this is all about behavior planning, which is a module of the self-driving car system that figured out where to go. And we're not together. And we'll talk more about all these things. Trajectory generation. This is where almost the rubber meets the road. Now we have to say, we know sort of where we want to go generally. But this actually generate a specific trajectory that tells us where the vehicle should go and how it should PRB and should accelerate or decelerate. And this is based on the behavior planning we did in the previous step. And finally, once we have a trajectory that we like, we have to send that to a control system which actually turns the steering wheel and slams on the breaker, hits the accelerator, so the cart moves in the right way. And so these are all the pieces of a self-driving car system that, or at least on the car. But there's a bunch of other pieces we'll talk about as well that are supporting infrastructure. Okay, without further ado, let's talk about semantic max. So what is the semantic map sounds exciting. Semantic maps, not just a map that you might have in Google Maps. Semantic maps are like a Google Map plus, plus here a little bit better. So semantic maps are autonomous, vehicle specific. In other words, they're designed for time as vehicles and their high definition maps. In other words, they don't have all of the road lines and cross-walk lines and everything else at, let's say a foot or two feet of error, they might have centimeter level definition or granularity. Resolution into the map so they know exactly where the cross-walk starts and ends because what if you had a foot of error for where the crosswalk starts and ends, you might have a vehicle that drives them to the cross-walk or drives partly in the intersection or something, right? So semantic maps are very high, deficient in very precise every line indicates where the lanes are to the centimeter. Now, semantic maps also have details on the exact location of intersections and crosswalks. As I said, again, because when you're doing a self-driving car, your human knows to stop behind the cross-walk, but a self-driving car needs to know exactly where it starts and ends, where the intersection starts and ends. Otherwise, it might drive too far and it hit. One thing that you might not expect that semantic maps would have would be the precise location of traffic lights and stop signs. So often, when you're driving, you might see a traffic light in that traffic light might seem obvious to you that, you know, it's red or green and you know whether you should look at that traffic lights move forward. But sometimes in an intersection you might have two or three traffic lights, right? One of them might be for the Leanne next year, the left turn lane one might be for you. What might be for the, you know, the, the perpendicular traffic that's going back and forth. And so you as a human, have developed an intuitive ability to look at traffic lights, figure out which one is relevant and use that one. Autonomous vehicle sees all the traffic lights. It's perception system C3a, four or five different traffic lights. Don't know which one is the right one. So these semantic maps tell the autonomous vehicle for every lane. This is the traffic light you should be looking at. Here's where it isn't space. So that when you perceive it, when the autonomous vehicle sees that it can say, Oh, that's the one I care about, not this other one next to it. Okay. And so you wouldn't think that this would be necessary, but it is because otherwise, the cars to figure this out into its own. Now, the cars like Tesla is, I believe, do you figure this out there? Are these figure it out and they make a guess, but they could be wrong if you have this information in the map, There's a much lower likelihood that you'll make a mistake, especially if there's two or three traffic signals in the same place. Speed limits to restrictions and lean direction. So the semantic map also says, Oh, guess what? This stretch of of, of the road is 25 miles per hour between this time and this time, for example, because that's when kids are present, then you may have to drive slower or there's no right turn at this time or this direction, this lane is a single direct direction laid it only goes in this direction. So all of these elements have to be the semantic map because the self-driving car needs this information. Otherwise you would have to figure it out on the fly as it's driving. And in general, you want to try to free program as much as you can for the car. So it doesn't have to figure it out as it's driving cars. Basically, these cars are supercomputers that are competing kinds of stuff. And the more you can offload, so they're not computing as the car is driving at ten times a second, the better. Okay, so semantic maps are super important. Now, this is just the first type of map that is used in an autonomous vehicle. Okay. Oh, I forgot one thing. This is also interesting. Oops. So parking locations, so imagine that there's a street and the street has parking from a certain time of day, let's say from 11 AM to 3PM, you can park on the side of the street, but during the morning rush hour, in the evening rush hour, you can't park on the street because we have to get more traffic of fluid, right? Well, imagine a self-driving car, 1130 in the morning is driving down the street and it runs it finds a parked car. How does it know that it's a parked car? Could be by just stopped behind that parked car and wait forever because it thinks that our car basically is eventually going to move. So one of the things you also put the semantic maps or parking locations like, Hey, between this time and this time, they're going to be parked vehicles here. And so you don't want to get behind those vehicles because you're going to be stuck for another six hours. So all of these things are required in order to optimize or make it easier for the car to drive. Because otherwise they have to be figured out a self-driving car to have to figure out, is this Apart car? And what, you know, and how long do I have to wait before I know it's a parked car. This can be pre-programmed are provided to the car, so it doesn't have to figure this out. Okay. So let's talk about LIDAR maps. So lighter maps, the second type of map used for the self-driving car. So first of all, what is lighter? So LIDAR is a spinning laser beam. So imagine if you were to take your hand and you've had laser beams coming out of your fingers shooting out. Okay. And imagine if I were to spin these laser beams around 360 degrees, ten times, okay, ten hertz. So as the lasers are going out, they're going to hit something and whatever the directional is just going to hit and they're going to bounce back. And depending on how far the leader has to travel, if it's traveling a 100 meters, going to take a little longer to bounce back that if it's traveling 30 feet and it bounces off a person and bounces back. And so basically these lasers are spinning around in a 360 degree loop every, every tenth of a second. And the bounce back of those laser beams is then collected by the Lidar. So you basically have a three-dimensional point cloud. It could be called a point cloud of, of reflectance of every laser beam in the environment. You'll see it bouncing off buildings and trees and pedestrians in cars and even off the road surface. It can sometimes even detect like road markings like the white paint on the road because they reflect more depending on what the laser beam bounces off of it. If it does stuff, something dark, it's going to get lower reflectivity than if it bounces off something light and shiny. Okay? And so imagine if I were to take a vehicle ahead of time, this is not the self-driving car. It's just a regular vehicle that I stick a LIDAR on. Okay. And I drive through all of the streets of a city ahead of time, a couple of months ahead of time. And I have this laser spinning around and spinning around. And at every point I collect information as to what beam is providing to me, what what the beam hits 10 times a second at every point in the city. Okay? One second here. Okay? So imagine, this is just an example. I'm showing five different beams here, but imagine that before the self-driving cars even built, we take a car, this yellow car, we put a lighter on its roof and we basically have a drive to illustrate. And let's imagine this car has very precise location information nose to the centimeter where it is right now. Well, you can get these bounce back of the beams and say, Oh, guess what? We saw a bounce back at 7.3 meters and about, you know, I don't know how far is that? A hundred and two hundred. 100 degrees back. Okay. And we saw a bounce back and like a 100 degrees forward at 17.4 meters and so on. So at what what the advanced vehicle will do that's driving ahead of time. It's it's kinda basically said each points at each location and particular geo-location. This is the bounce back I saw. Ok. And it's going to do that at each latitude, longitude with ledger, with lasers. Okay. And then it's going to do in the next position and it's because I got different bounce back when my geo-location was this latitude and longitude. And then it's going to do it again at the next location. Now, what's interesting is it doesn't just do this with five lasers. There's a 100 thousand points every time the laser spins every tenth of a second. Okay, so a tenth of a second, you're getting a 100 thousand laser beams bouncing better beams bouncing back, which are indicating the range to various things that your windows on buildings, trees, and so on. All this information is saved into a database of terabytes and terabytes in size. Imagine a 100 thousand points, 10 times a second at every geo-location as this advanced cars driving along the streets. Now, basically, lighter maps are used to determine egos precise position, and we'll see how this is done later. But what you might be wondering, is it possible for a self-driving car to drive if they haven't mapped all these lighter bones? And the answer is, for most self-driving cars, no. A self-driving car can only drive where earlier a team of people have driven the car around and collected this LIDAR data. So self-driving car can use this to figure out where it is. Okay, So these lighter maps comprise terabytes or petabytes of data. A 100 thousand spins, ten times a second million spins a second of reflectivity information associated with each geolocation on the rap, empowered in the entire city. And so virtually all time and vehicles, except for Tesla again, use semantic and lighter maps. They use both semantic maps and lighter events. Now Tesla is outlay or they're trying to figure everything out and do it online just like you would do as a person. You don't have maps in your head. You just drive and figure things out as you go. You don't know where the traffic lights are. You don't know precisely where you are. The centimeter, you're just sort of figuring out and adjusting. And that's what Teslas are trying to do. And the question is, are they going to get there faster than everybody else? Okay? So, but the other autonomous vehicles to Zeus, Waymo and so on. Level five, all the, of, all of them can only drive in regions where they already have a high definition semantic map and these lighter metals. Okay? Alright, so next let's talk about route planning. So route planning is all about figuring out, like Google Maps, where do I want to go? And so generally speaking, an off vehicle system is going to figure out the route. You're not going to use an on vehicle system to do that. You're going to go to a data center just like Google Maps. You going to say, here's where I am now. Here's where I want to be. Come up with a route for me, Okay, and make it optimal. And these are done offline, just like, you know, you might do with Google Maps because they can take into account traffic information, real-time information. There might be construction. They can figure out exactly where the vehicle should drive. Now, why is it so important to know about live traffic information, live construction that's happening right now, or maybe a traffic stop that's slowing tropic. Well, let me tell you, it's very difficult to build a self-driving car. And so if you know there's construction work on a particular road, even if that road might be on your path, you want to avoid that stretch of road because it's much easier to build a self-driving car that works in a very simple circumstances than one that has to deal with a construction worker holding a sign saying slow or directing you want the opposite lane of traffic. It's very, very difficult. So part of what self-driving car companies do is they try to find routes that minimize the amount of chaos in difficult situations at the carts to deal with. And so having a route planning system that can take all these things into account can minimize the complexity that you have to deal with. Because building a car that can deal with any circumstance like, Oh, there's a crash in the lane and you have to go in the opposite lane or, oh, there's construction work and there's somebody you have to follow a vehicle through a single lane in order to get to the other side and Tom feel cold, can't figure that stuff out. So you just try to avoid. So the route could take into account again, real-time traffic conditions, construction, and so on. And writing instructions might include details on which lanes need you need to be in as well. So imagine that I'm going to need to make a right turn in half a month. Well, you don't want you want to tell that to the time as vehicles part of the route, the route information so that it knows it switches lanes into the right lane as early as possible because it's very difficult for an autonomous vehicle at the last minute to swerve around other cars and make a right turn. It would be very difficult. So you want to give it this information as early as possible so we can say, oh, over the next half mile, I have to consider making a lane changes in the right lane because otherwise I won't be there in time and it can then plan ahead of time what to do. So if you're interested in turn by turn navigation, it's actually not really difficult. You might think this is super difficult to do. But in fact, we've done this since, yes, 32 and winner of 2020, which is two years ago. Now, our students build turn by turn navigation from scratch. And they use something called OpenStreetMap data. So OpenStreetMap data, you can look on it online. Openstreetmap.org has basically stream map data for the entire United States, maybe even the world. And literally it's not centimeter level, but it's, you know, meter level street map. It says This streets connected to the street and there's an intersection here connecting these other three streets and all this information. You can actually build a program and a couple of weeks or laughs to build turn-by-turn navigation. If you don't believe me, ask the juniors now because they built this a couple of years ago from scratch. We gave them the data and they then built turn by turn navigation. A star is an algorithm that's used for doing this routing. And it's not that complex. It's not that complex. We'll learn a little bit about this in CS 32 and you can figure it out yourself by just go into Wikipedia, right? Yeah. All right. So perception, Let's talk about perception. So perception is all about perceiving your environment, knowing where other agents are in the environment. So other agents might be pedestrians, pets, vehicles, you know, bikes, scooters, mopeds, and so on. And perception is done using three different modalities, primarily cameras, which you're all familiar with on the left. Radar, which is used to provide a little bit of a growths or signal. Like it's not very precise, but it gives you, says there's something over here and it's going at the speed. And then lighter as we talked about, which is spinning laser which gives us very precise bounce back and every point did it hit the rear view mirror on the car or the sideview mirror? They hit the tail light. You can literally see that little detail with these lighters. And what autonomous vehicles do is they use what's called sensor fusion, okay? And sensor fusion means they don't just treat these these inputs, the visual camera and the right and lead our radar and lidar separately. They actually use all this information together to generate a bigger and better, more accurate picture of the world. Okay, So sensor fusion is about combining data from all three types of sensors to improve recognition. Because you want to recognize things better. And if you actually have visual information and laser bounce back on, for instance, an object you're going to know it's a car and the car doors open, for instance, much better than if you just use radar or just lighter or just came. Right. Now, you might have heard of this topic or this term. It's called convolutional neural networks. Convolutional neural networks are really big and self-driving cars and in general, in computer vision. And what they are is a neural network, an artificial neural network that can be used to recognize objects. Okay? And so the way it might work is we take a picture, okay? And if we provide a picture of a car, it feeds it into the neural network. There's multiple layers in the neural network. And the neural neural network can basically say, oh, it's a car, I got it. Okay. So convoluted convolutional neural networks take an input image. They can also take lidar information and radar information. They throw it through a bunch of layers of the neural network, just like your human brain, has a neural network with multiple layers of neurons. And then the output, a category like this is a car, a pedestrian, and so on. And the location of this car is right in front of me or this pedestrians to the right. In this set of this location of the image that I see on the upper right are the lower left or whatever it happens to be. So how our convolutional neural networks train. What you do is you take millions of pictures from the road, okay, of, of other vehicles and pedestrians walking by and cyclists and cyclicity in turning in front of the car and going and taking a right turn and driving alongside the car, every possible scenario you can think of. And you give those images to humans who actually label them and they say, oh, in this picture that was taken by a vehicle is driving around. We just had a vehicle driving around with cameras on it. In this in this image which was taken six months ago, there's a pedestrian in this little box here on the upper right. In this image there is three vehicles in the lower left, in the middle, in the upper left, and so on. And so basically humans annotate these images by drawing boxes around all the humans and dogs, and cats and people and mopeds and everything else. And then you basically feed all that information and train a neural network. You say, here's the input image that a car might see. Here's what a human's said. You should learn from that image that you need to find a box here that says this is a pedestrian and the neural network learns this without being programmed. It just learns by being provided with examples of the right answer for each image. Okay? And so these neural networks are extremely powerful, they're very effective. And just a decade ago, maybe 10, 15 years ago, if you asked somebody, could you given a picture like this picture of the cake, the camera on the left here. Okay. Give it a picture. Could you identify a car or a pedestrian or a tree? People would say Tell you are crazy. Now, you'll never be able to do that. These neural networks are now as good or better than human beings and identifying items in images, they're really, really good. And ten years ago, we didn't even think we could do this. It was even possible. So this is a major event. Now, once we've identified all the objects in an image by It's using a neural network, a convolutional neural network and giving it the LIDAR data, the camera data, the radar data. We then have a bunch of objects. Okay? But guess what? It's not just enough to know that there's a car here and a pedestrian there and so on. We have to now know that over time, as I look at multiple images over time, that this car is the same car that's in the next frame and that's the same color, That's the next frame. I have to track these objects over time. And the way this is done is with something called Kalman filters. Well, these neural networks for this too, but common filters. So common filters help track objects over time, giving them a consistent identifier from frame to frame. So why do we need to just identify from frame to frame? Well, if the autonomous vehicle just knows there's a bunch of objects like a car here and a person there at time 0 and time 1, there's a car there and a person there. Guess what? You can't drive a car. You need to know that the car and this frame is the same as the car, the next frame to compute the velocity of the car and predict where it's going to go in the future. If you just say, oh, there's a card here and there's a person there. Then how do you drive a car? You need to know that this entity at time 0 is the same entity is this entity at time 1, time 2, and time 3. So you can predict where they're going to go. And so a common filters do is they help track. This is called tracking. They help track where objects are and where they're going over time so that we know that this object is the same as this object and this next frame and this object to the next frame, so on, they're all the same. So we can compute velocity of each object, an acceleration of each object over time. And it's not as if it's just a bunch of random object over time. That's called tracking. If you want to learn more about Kalman filters, you can look them up. They're, they're a little bit confusing or difficult to understand. But you can find some good YouTube videos if you want. And these are used all over the place, the pharmaceutical. Okay? So this is necessary to compute object's velocity and acceleration. Without that, you could actually drive a car. And you can read more about this. There's a PDF and so on. Okay, so that's perception. Now once we have perception, we know where all the other objects are, all the other objects are, and we know where they are over time. We know that this object has, is following a path over time because we have a tracking that we just talked about. Localization is all about figuring out where ego is. So you might think that these cars can just use GPS and they do use GPS, but GPS generally has 5 million meters of accuracy cribbing. Why is this? Well, the government, when they, when they, when they send these GPS satellites up, they didn't want me like bad people to use these to me. I'll make laser guided bombs and without centimeter level accuracy. So they actually limited the accuracy of GPS systems so that they can't really be used for very precise tasks. So you can't just use GPS to drive the self-driving car because it's really inaccurate. Imagine if you said okay, I'm going to just drive with five meters accuracy. And then the eagle vehicle would then drive into the intersection because, you know, most crosswalks, you're not five meters wide. So you need more than that. Do you know when there's a low? The government doesn't want you to give you the size, the precise location, size solution. Although the precise with subgroups, yeah, they they supposedly have much higher resolution, much, much higher. Yeah. So now coming maple Tesla. Hello, going to slug the sky with this loan is certainly true. If they want to, to, to, to develop a high GPA for themselves. The law will be thanks for doing this. I don't know. It's an interesting question that that flowing it's above my pay grade. I don't know what the one about being a PE, so definitely above my pay grade, but but I couldn't tell you whether or not this would be forbidden or not. But the horse is out of the bar. And let's just say because you know, there's always Microsoft's now that are being created. It's very easy for anybody to do that kind of thing. I don't know how they would stop if they if they could do this, this will be also a huge advantage. Well, it might be although they don't use this information now they're not using High-Definition live without this right now. So that's impressive. Yeah. And one thing you're asking about is that they have tons of data. And so we can talk about that later, maybe about how having tons of data can actually have a big impact on your ability to drive, because this is a big data problem. You need tons of images in order to train your convolutional neural networks in order to recognize things. And this is a big data problem. If you have data, you will win this game. Okay? So what's this? So we see on the right hand side, this is called an IMU. And an IMU is like a gyroscope. Imu stands for initial measurement, measurement units. And what they do is a measure rotation, acceleration. All these things just like a gyroscope might. And this is useful to provide another signal, Diego. So for instance, if we, if, if the vehicle slams on the brakes, well, if you know that, we can actually detect that with the accelerometer, an IMU, which then tells us, oh, it's slowing down. What if you're in a tunnel and you don't have access to GPS signals that i and u can be used to basically help you figure out where you likely going to be the next second or two seconds or three seconds. And so these vehicles use something called dead reckoning where they basically say, well, I know where I was a second ago. I know that I'm decelerating because the IMU detects this deceleration or detects it curvature to the left or the right, like a gyroscope. And it can then estimate where the vehicle is again using what's called a Kalman filter in order to, in order to get a better idea of where the vehicle it. So the moment you have a precise location using the accelerometer can help you figure out where you're likely to be over the next seconds with reasonable accuracy. And this information is fused together. Okay? Now, the other thing that's used, remember we talked about those LIDAR maps that are built ahead of time by people just driving around the streets in the city that you want to train your car for. Well, if you recall, we built or are people belt and these localization maps, these LIDAR maps, where they basically at every location on a route or every location on every street in a city. They have the 100 thousand laser beams that that bounced back at that precise geolocation and that precise centimeter level location. Imagine ego is driving now, so he goes now self-driving car driving down the street and its laser spins around and gets a 100 thousand readings. Okay. So is it gets those readings. It says, well I got the following bounce back. So what ego does? And this is amazing. They can do this. Maybe this is a very computationally difficult problem. It says, these are the bounce backs I got right now. It compares that against all the bounce backs that were collected during that phase when these people were driving the streets of the city and says, guess what? That bounced back I'm seeing now is closest to the one where I'm at, at, at particular geo-location like 33.79 by minus 180. So imagine, think about think about this. You're saying based on the laser readings I'm getting right now while this ego car is driving around, you compare that to the laser readings from every point to the city potentially to locate the one that matches the most closely. It looks like both booth looks closest to the bounce back you start earlier and that's then used to figure out your precise location now. So how would you build that? Think about how difficult that might be if you had to go and take a 100 thousand points of why with distances from you guys say 10 meters for this one, it's reflecting this much and three meters from this direction, and it's reflecting as much at the site. And then compare that against. Imagine if in a typical city we might have a couple petabytes of data, petabytes of 1000 terabytes, which is a million gigabytes of data. To figure out which of the spins the recorded earlier is closest to the spin you're doing right now to figure out where you are precisely on the ground. This is called LIDAR localization. And there are algorithms for this. And what's even more interesting is the spins you might get while you're, while you're driving or not going to be precisely the same, isn't that were recorded a couple months ago because ego might be five centimeters to the right of when the original signals were recorded or my maybe 1017 centimeters to the left with these, with these little laser rate readings are taken. So they have to occur for all this and say, well, you know, I'm still pretty close to this. And so therefore I'm probably at this location. So this is light our base localization. It's very interesting problem. It's computationally very expensive. And this is where real computer science come. Then you can't just do this by writing some simple software. So the vehicle fuses all of these Roosevelt's together from the, from the GPS, from the lighter localization from the IMU and it, through all these things, it then gets centimeter level accuracy, which is necessary to drive the car. And again, you can look up particle filters and common filters to learn more about these things. Particle filter is another type of filter that's used to fuses information together. So that we can get a more accurate reading that all, neither of them are there. Okay, so what's prediction? So when you drive a car, you are not thinking about where each vehicle or pedestrian as now you're thinking about where they will be in the future because you are driving a car at 30 miles an hour, 60 miles an hour. And you have to have to have an idea of where these other agents or vehicles are. In the next 10th of a second, second, 2 seconds in order to safely drive your car. And prediction the prediction system and an autonomous vehicles all about figuring out where all the other agents are going to be in the next five or ten seconds or a couple of seconds, okay? You have to know that in times vehicle. So what we see here is a, a, a, this is called a raster. Rasters. A bitmap basically show the top-down view, a bird's eye view of a scene. And you can see the light, the white here, it indicates a vehicle at the current time t equals 0. And the greyer, darker grays indicate the vehicle at previous points in time. So this might be a t 0, t minus 1, t minus 2, t minus 3, t minus 4, maybe tenths of a second for something. Okay? And what a prediction system does is it takes in the previous locations of a given vehicle like this one here on the left. So it says, Oh, guess what? Over the last 2.5th, it was here, here, here, and for the longest to go here. And then it predicts potential paths for that vehicle in the future so that we can use that in order to figure out where the vehicle is going to be and navigate around whether people will be, because you don't navigate where ground vehicles are, agents are, you navigate where they're going to be based on where you're going to be in the future. It's a space time problem, not just a speech problem. And so, so basically, a prediction system will take the history of every agent in a scene, an agent being a vehicle or a pedestrian and cyclist. So the history might be these four locations of the car, 500 kids in the car, and then uses to predict likely trajectories in the future. Okay, So there might be two potential trajectories that we expect for this card. My turn right, might go straight. Okay? Now in this diagram, you see these circles here. See those circles. These circles represent uncertainty because you know, what, we're not really certain is a vehicle going to take a wide turn? Is it going to take a narrow turn? Is it going to slow down or speed up? Because this is not just about space, but space and time. So we don't know if the vehicle is going to be when it's around here. It's going to be at the beginning of this or at the end of this week because it's going slower or faster, is it going to take a wider turn to be towards the side here, or a narrower turn it towards this side here. So uncertainty allows us to know that, you know, we can't really say whether the vehicle is going to be any, you know, where it's going to be. It's going to be probably somewhere around here with a high degree of certainty. But we can't say if it's going to be the middle, left, right in the beginning or ending and it gets, the uncertainty gets wider as you go out because in the future we don't know maybe the car is going to slam on the brakes three seconds out. Maybe it's going to accelerate, maybe it's going to swerve because it sees a cat. So basically the prediction system says, this is my best guess at where things are going to be in the future as opposed to this is exactly we're going to be because it just can't know. Okay? So the uncertainty gets wider as we go out or bigger as we go out. Now, the predictions are done in often the following way. What you do is again, you take a bunch of data from vehicles that have driven around in the past, over months and months or years of time. And you basically build a bird's eye view map like we have here, where you have all the vehicles sort of rendered out from a top-down view. And this is, it's possible to create this because you actually know where all the objects are. Because of these vision perception systems, Lidar systems, right? So we build a map like that. And we actually know in the past where a given vehicle was over time, the last five seconds or whatever, where it went over the next five or ten seconds. And we can actually train a neural network, for instance, to say, Oh, Well guess what? If a Vehicles slowing down in the last five seconds, it's likely to completely come to a stop or if it's speeding up, it's likely to go straight, or maybe it's likely to turn right. And so basically you can train a neural network just like we did for the recognizing images to figure out exactly what we're not equal, Not exactly, but what's probably going to happen in the future for each vehicle by giving it millions of examples and having it learn. So o by the intersection at my turn right, go straight. It's slowing down. It's probably going to turn right or oh, it's speeding up. Probably going to go straight. Okay? And so these, these systems use neural networks and probabilistic reasoning. So what did he buy probabilistic reasoning? Well, imagine a car at k minus 1. K minus 1, we don't know whether this is going to turn right, it's going to go straight. We don't know. So we have distributions, these distributions here which are like Gaussian distributions. There's some probability of getting this and turning right, some probability to go straight. But over time as the car moves, the position on this distribution changes. So it actually can figure out, Oh, guess what? It's slowing down as it seems to be angling to the right. So therefore that there's a higher probability that we're actually turning right. We're going straight. And basically you can reason over time as you get more information, what's the likely future behavior of this vehicle that you can't know precisely, but you know, you can basically compare a versus B, a versus B versus C with probabilistic reason. Okay, so good at stuff like this. So you can play, this is where these loci we choose. Why did you use a size circle? On the other hand, you say good, very good. To pull on. It. Doesn't know what the rack. So we're going to compute the new merged to do because the voyage each other. Yeah, I don't know the size of each other. It's a great question. So what you're saying is, look, some of these problems can be solved entirely with neural networks. Some of them today can only be solved with things like Kalman filters. And some of them are problems where there's research going on actively to throw away things like Kalman filters and just use a neural network to do the same thing. And the answer is like, how do we know what to use in a self-driving car? The answer is that it's constantly changing. It's a bunch of hacking and playing around and saying, Well, we can't get good accuracy with a neural network right now. So we're going to use a Kalman filter for this part. We're pretty good at recognizing vehicles, so we'll use a neural network for that part. And basically they build a system that does it partly with common filter and cart, partly of neural networks based on heuristics. They basically found that, well, this looks bad, Beth. And then somebody does some research and says, Well guess what? I can do a better prediction of tracking with the neural network then they can do with the Kalman filter and they throw away the common filter and they used to neural networks. So it's constantly evolving. There's no right way to do this. And there's no science that says, this is how you connect these two things. People basically use their best guess to figure something out. They try it, they measure it and so on. Yeah. Okay. So, so basically prediction is providing space time information on every agents in the scene where each agent will be at a particular time in the future. And that information is sent to what's called the behavior planner. So behavior planner, it, its goal is to basically propose one or more high level maneuvers for you. Should I go to the left lane? Should I keep going straight? Should I make a right turn? Should I stop? Should I go forward? Okay? And the behavior planner is also trying to come up with a set of constraints that can basically allow limitations on those maneuvers. Okay. So ego might go straight, it might go right. Michael led to the left lane, make a left lane change. That's one thing that behavior planner does. It figures out what are the major high-level behaviors and I'm going to want to perform, okay. Okay. And to do, to do this, it also needs to understand where other agents are going to go. So it needs to take the predictions from the prediction system and say, Oh, guess what? These vehicles are probably going to do the following things in the next couple of seconds. Okay? Because how can you propose a behavior if you don't have an idea of what other agents are going to do in the environment in the future, okay? And it also has to take into account traffic elements. So if it seems a yellow light ahead, it's going to know, it's going to need to use that to basically pre-planning behaviors. Well, it's going to say, Well, I probably want to plan of stopping behavior as opposed to going behaviour or if it sees a stop sign, no matter what the other vehicles are doing, it's going to want to plan a stopping behavior, right? So it needs to take into account traffic elements, traffic lights, signs, speed limits, all of these things need to be taken into account. Okay? It also needs to plan in static objects. So for instance, how do you plan? So we have a car here, for instance, on the side of the road, cones in front of it. It has to basically understand that there are static obstacles that you're going to need a drive around it as well, okay? Now, it takes all this into account the behavior plan or generates what's called a constraint map. Okay? So what does a constraint meant? A constraint map is a set of polygons, baby rectangles in space and time that specify desired areas for the vehicle. In other words, this is where we want to go straight or we want to make a right turn, or we want to go left. So here are, here is the rectangle at time t equals 0.1 seconds where we'd like ego to be. Here's the rectangle at 0.2 seconds where we'd like you go to B and so on. So positive constraints on where we'd like the vehicle to be. And also negative constraints. In other words, hey, here is where we think another pedestrians going to be in the next few seconds. So you want to avoid this region ego and you want to prefer this region. So let me show you this map here. So this is a space-time map on, on the CSWA, on the L and S axes is just like X and Y, basically sort of like x and y. And on the vertical axis is time. Okay? So this is a spatiotemporal or spacetime. Basically set of polygons. And here's ego right now. So you see, you go right there and you go, it wants to do a maneuver and maybe ego wants to go into the left lane, okay, Well, guess what? There might be a set of polygons or rectangles that indicate that if we want to go in the left lane, here are the positions generally we should be and we want to prefer these. So there might be a bunch of polygons to show that. But we also have other agents like this agent here who's making it going doing Laughlin change as well, for instance, or this agent here. And so we take the predictions that we had which tell us where the other agents are going to be in space-time. We say look at 0.2 seconds in the future. We cannot be in this region because if we are, we're going to collide with another vehicle. So the output of a behavior planner is a set of constraints, polygons in space and time, which indicate where we like, what we want to be and where we absolutely have to avoid those polygons take into account other agents and where they're going to be in the future, where we think they're going to be. It's going to take into account static obstacles like a parked car or a stopped car, or a pedestrian who's standing in the middle of the street, not be maybe they're lying on the ground. Maybe there's a dead animal from the ground or something. All of these things we generate constraints are saying you can't be at this position at this point in time. Now, these constraints are going to be used by the next part of system, which is the trajectory generator. But let me tell you something. When you plant behaviors, this isn't just done once. This is done 10 times every second, because everything is constantly moving. All the agents are switching lanes and they're looking down at their watch and they're spinning around and the bicycle is swerving. So these constraints that identify where you want to go and where you want to avoid are changing 10 times a second and the cars replanning constantly. It's re-planning this and coming up with new sets of constraints over time constantly. Okay? And you can read more about this in this article, and I'll provide you with his PD with the PowerPoint if you want to look. Okay. So now we have the, we have a set of constraints that say you cannot be in this rectangular region of five by seven meters at this point in time, because otherwise you'll end with a car and you can't be in this region. If you're in this region, you're going to hit the curb. If you're in this region, you're going to go past the stop, sign. It. But this region is reasonable because we want the car to go here and so on. So after all of those constraints are generated and again, their space time constraints, they have X, Y, and Z information as well as extense ranges like how wide they are along there. We then give that to put trajectory generated. And what the trajectory generator supposed to do is basically, it generates a preferred kinematically feasible trajectory. Kinematically feasible means CAPTCHA actually do it because you can't just turn 90 degrees in a second. You have to go in turn slowly, otherwise a Carl jerk or it'll just stop. So you need to actually generate a trajectory that it can actually be followed by the car. And this trajectory might be based on your previous trajectory of a last second or two. Because guess what? You don't want to take a trajectory that's wildly different than the one you were taking. The car is going to jerk and so on. So you take into account the path of the car, the speed of the car before in generating new trajectories. So maybe that's a trajectory we want to arrive at. That's what we want because it's safe and it avoids the future positions of all the cars and so on, based on those constraints from the behavior planner. Okay? And the way this is typically done is actually a trajectory generator will generate thousands of random trajectories. You'd be surprised and computer science how much randomness is used, but it's used a lot, okay? Randomness is a fundamental approach of solving problems computer science. And in a self-driving car, they will generate tens of thousands of random trajectories, some of which are going to drive off the road, some of which are going to drive into other vehicles. Some are what you're going to hit the curb, some of which are going to be perfect to navigate the car. Some of what you're going to go into an intersection, even those are red light. These trajectories are all randomly generated and they're based on the previous trajectory that we had before. So they're consistent. They're not going to cause a card a jerk. Well, okay. But some of them might cause the card a jerk a little bit and some of them are going to be like little bit infeasible or difficult to maneuver. And these trajectories are then filtered against the space time constraints that we generated by the behavior planets. So the inner planets, they know you can't be here the next 2.5th because there's going to be another car there. So that's going to filter out or weed out some of these trajectories that are not feasible because they're going to run into another vehicle or pedestrian. Or we think they're going to be at least now. Okay? And so these constraints are used to then filter these trajectories using things like a GPU graphics processing unit, which can process many things in parallel. Because imagine how long it would take to take all these face time constraints, hundreds or thousands of polygons, and then compare them to tens of thousands of individual trajectories which are candidates, okay, in order to find the right one. So this is done on GPU hardware like the seniors for mining Bitcoin in order to identify the one best trajectory that maximizes our path for where Eagle wants to go based on our route planning and minimizes the chance we're going to collide with a prediction, predicted vehicle location of the future or go past a stop sign. Dr. Into a red light and so on. Okay? And that trajectory, ideally it's going to also taking them out into account. Kinematics like is it, Is it comfortable? Is it going to cause a lot of dirt to left to right? It's going to accelerate because this is not just a 2D trajectory. This is a trajectory in space and time. It's going to say, Hey, accelerate here, decelerate here, turn left here, turn right there. This is a space-time trajectory and so you have to get it right because otherwise it can be really, you know, if you slam on the brakes right now even though if you follow this path, it's going to be really uncomfortable. So generating trajectories, evaluating against the behavior constraints is the way that we identify the one trajectory want the car to take for the next 10th of a second. It's about every 10 hertz. 10 hertz. We plan. Once we have a trajectory, we send that to the control system. And that basically then tells the steering wheel how to turn and the how to put the push the accelerator, the great. Okay. And so the control system translates a trajectory in space and time into a set of steering acceleration and breaking commands to the car. This is a difficult prominence own right? They use something called a PID controller for this. I don't remember the math I looked at. I looked it up on and wanted to interview for the job. But this is a whole part of control theory. This is used in robotics, left or right PID controller. Pid controllers help smooth out commands to the car. So when you're driving, it doesn't jerk and basically transit steering wheel just a little bit rather than a lot. The PID controller implements the commands to the car, the actual physical car based on the trajectory. You can read more about this learnt look about look up PID controller. Okay, we're almost done here. And another tunnel lot of time for questions. I can stay later if we need to, but really quickly so we can talk about all the stuff with the vehicle. But there's a bunch of other things that are super important for self-driving car. Simulation is one of them. Simulation is all about simulating how the car is going to do in simulated Live scenarios. For instance, let's imagine we recorded a month ago navigation where the car was driving and a bunch of other cars came. You can actually simulate what happens if the car behaves in a different way based on the new software we've developed this week in that original circumstance. And so simulations super important for that, for evaluating how well the car is doing. Is it driving well as getting in crashes. Before you release a new version of the software, you want to simulate that movers and the software in a million scenarios to see, does it do well? And this is again, running on very expensive GPUs. You running the actual autonomous vehicle sac in a simulated virtual world. You're simulating other vehicles reactions to the ego and so on. It's very interesting. Now the other thing simulation is used for is for training the car. Because a new way that these systems are being trained is using something called reinforcement learning. Where reinforcement learning is you actually try driving the car and try different maneuvers and see what works. And some of these maneuvers are going to cause a crash. Okay? And so in simulation you can actually observe the crafts, realized that it's actually going to cause a problem and then train a neural network, avoid this maneuver and this circumstance because it will result in a crash. So simulations used not only for evaluating how well the new software is doing in the car, but trading by trying all kinds of crazy stuff and having the Carla and just like you do when you're first learning how to drive, you learned by making mistakes. And actually you can trade a vehicle using neural networks to do this as well. With reinforcement learning, metrics, you have to measure things. How are you, How are we doing? How often do we run red lights off and we'd drive into a curb and simulation, or in real life, how often do we slow at a, at a, at a rate which is too fast and so it feels jerky or harsh on that on the passenger. You have to measure these things in order to actually improve the system. And if you don't have metrics, you can't build a self-driving car. So it might seem sort of boring, like, Oh, I have to go and track how well I'm doing it stopping at stop signs over time. And that's not really that interesting. But if you don't do this, you can't improve yourself having carcass. You don't know whether doing worse or better at each point in time. Cybersecurity, no doubt there you don't want your year 33 ton or two ton vehicle to be hacked and then controlled by an attacker. Validation, validation is all about validating that the car is actually safe and ready to be deployed on the road. So you have to validate it. You have to set criteria. We can only go and potentially have this many faults per unit, per time per month of simulation or we can't release the car. Now of course, their safety drivers monitoring these things in most cases. So the safety driver takes over, but you want to validate, is a car performing under a particular set of scenarios as we expect to do with failure rates that are reasonable. And if not, we failed validation. We allow it on the road. And then machine learning infrastructure. So all of these, all of this machine learning, we talked about training the machine learning system. You're training these neural networks requires a set of infrastructure that typically runs in the Cloud and Google Cloud or Amazon Web Services, where you actually take a billion, a million images. You take these annotated curated images and train. This is what you should expect if you see this image here is the pedestrian in this image, here's where they are. You need to build systems to below those things. And there's teams of people, dozens or hundreds of people to just build the system that we're talking about, who to train these neural networks. So these are required for self-driving cars. Okay, final topic is the trolley problem. You may have heard about this. So the trolley problem is a series of thought experiments. That involve ethical dilemmas, okay, and it was developed by Judith Thompson in 1970s. So imagine we have a trolley that's going to go and hit five people that are on a train track. Okay. And if you do nothing, it's just going to keep going straight. The trolley is going to just go and get five people. Now you're at the switch. You can throw the switch and actually divert the trolley onto the fact that you do so you know that the trolley is going to hit on a different person. So you as a person are making a decision to kill the person on the top track to avoid killing the five people on the bottom. Fact, saving a net of four lines. Okay, Now this might seem like an easy problem for you, but do you want to be the person that decides that we're going to kill the person on the top as opposed to the five people at the bottom? Well, this is a hard problem. This is an ethical dilemma. What if it's a baby versus a, an older person? Which one? If you have to hit one, because there's no way to stop the current time because a dodge and for the car, you have to make an ethical decision. And this is part of designing a self-driving car. You have to go and prefer unfavorable things. You have to basically solve these problems because otherwise you can't build a self-driving car. And so this is an area of active research. And I think that there's going to be, you know, people are going to figure out what is the right thing to do here. If we do this right thing and we still kill somebody heat with the car, is that deemed acceptable because we save these for other five other people. This is a problem that also has to be computed, okay? And that's it. So now you know everything that it takes to build a self-driving car. And here's the actual logic. I'm going to show you some intellectual property. So if going hit Stop, Don't. That's like basically the logic. It's that simple, I think. Ahead. So if only that were the case, I hope you enjoyed the talk. Obviously, there's a lot of interesting things to do here. Go get an internship at the self-driving car company. Have a lot of fun. And if you enjoyed this talk, come take my class calendar. This was a couple years ago. I can't wait to teach again in person because I hate teaching virtually. But I'm done. I finished in exactly one hour. I'm sorry. No questions. Thank you. Kerry, there. Any questions? I can stay longer if you want. Okay. Let me check. Maybe I have a question. Yeah. So I know the semantic maps are pretty much used for redundancy. But what happens if, for example, like one car or it goes through and it generates symmetric map for one area. And then it comes back like a week later and it's changed. And is it possible that whenever create a problem because it might trying to pair two previous data outdated because maybe there was construction or it's maybe there's more people or something like that. Yeah, so that's exactly right. Semantic maps are outdated and this is one of the problems with building a self-driving car is that you might have some construction or an accident just right now. And your semantic map is going to be wrong for that, there might, you might have taken down a light pole because of the crash and now all of your light are bounces. It used for lighter map, we're going to be wrong and everything else. So it does happen. This is why the cars still have to have a lot of smarts of them. You can also have things called tell operators, which people, right? Basically if the car see something, he just doesn't know what to deal with the semantic Nazi block, you can actually send live video to a person and you can then say, Okay, here's the right thing to do. Maybe I'll go start making a left turn here are navigate with the other lane because I know that's confusing, but this is a way to deal with it. But yes, this is again, a very hard problem and you have to be able to operate without updated semantic map information because it can be incorrect. What else? Got a question here in the chat? What's the difference between, right, radar and lidar? So radar is a very gross sort of beam of energy, as opposed to lighter, which is laser beam. So radar, radar was like a microwave oven. It's just basically send out lots of ways and then seeing what it gets back to. You see blobs for radar, lidar use you literally light beams. So you can literally see a beam bouncing off a person's side view mirror, or 50 beams bouncing off sideview mirror and you can actually see the shape of sideview mirrors. Radar couldn't show that. So letters much more, much more fine grain. But radar might work better in rain than LIDAR, which might hit a raindrop and then bounce back. And so they're complimentary. They're not, they're not one is not better than the other. What else? Come on, you got questions. Okay, House and Senate semantic maps generated or they've done automatically. So that's a great question. This is IP, intellectual property that companies basically build. They might use self-driving cars that are not software because they might use human driven cars with lidar systems in and other systems, an image systems, if they're going to probably start with something like OpenStreetMaps data and then they optimize a day. So oh, guess what? This, this, this point on the map is actually off by half a meter. Let's adjust it. So this can be done in an automated fashion. It's done in a manual fashion, is done with machine learning. It's done with lots of different techniques. It's difficult, it's very time-consuming. What else? Turn your video on everybody, I want to CVR ask them questions. I don't buy. I do throw up Pop-Tarts but they don't bite. Come on. There we go. Anybody? Rory, another question. Yeah. I said yeah. So because obviously the cars have a lot of this like a lot of things to do in terms of communicating with the servers and communicating with all sorts of different things to make sure that they can try it reliably. How can you make sure that those connections are like 100%, don't fail so that there aren't problems. Yeah, So generally beyond routing, well, there's no way to make sure that things are a 100 percent failsafe. By the way, fail-safe systems are a big part of self-driving cars. I didn't even cover that, but there are redundant systems and every self-driving car. So if the mean computers, golf line, they have a backup system that can go and basically taking the side of the road in a slow way it safe, you know, and honestly DRY perfectly, but just get you to a safe stop there probably. And I don't know because I haven't worked other companies, but they probably have a Verizon and AT&T, T-Mobile. And they basically use multiple communication mechanisms with multiple chip sets so that they're independent in case one breaks. But you have to have redundancy and the vehicle has to be able to handle these things even if the connections are not available. So for this, other than routing, you shouldn't need to talk to the mothership too often over the internet, over the wireless Internet in order to drive the car. Now, if you want to have a teller operator, the teller operator is not watching the vehicle was driving all the time would be too expensive. So basically the car saying, ooh, something is really weird here. I don't recognize what's in front of me. Maybe it's a tractor, then it would go and try to contact that person and hopefully, you know, most of the time that's going to work. But you don't need to have a connection to a tall operator all the time. All the vehicles, right? Yeah. All right. Somebody else? Yeah. Bread and he do brent, the shot. Maybe have any questions. Come on. I have a question about the ethics of fighting. We shrink. Essentially, he had a parent bank that I'm becoming like discriminatory. I like making sure that each ethnical, like how would that objective maybe picker me? Yeah. Well, that's a good question. I don't think there's any objective. There's no objectivity in this. Anything is subjective, right. Due to the baby over a middle aged person or over an old person. Do you choose a person on a bicycle versus person who's walking? There are some ways of doing this, but you can basically say what's the collision impact that you might have. For instance, if I'm going to hit two different people and one I'm going to hit sooner and what I'm going to hit later, but I'm going to definitely hit either one of them. Like has no way to stop. You might choose the one that's further away because you can minimize collision energy. Okay. But when it comes to like how did decide to equally distant people, one younger when older, one disabled when not disabled, man versus a woman, whatever it happens to be, these are ethical questions that people are going to discuss and debate. And ultimately there's no probably perfect answer for these things. Just going to, everybody is going to be unhappy with some of these decisions. But then my guess is that this will eventually be done in a consortium. In other words, Waymo is not going to do their own and Zeus is not going to be there. They're going to all work together to say this is what we say is acceptable. And when the inevitable happens in the worst-case happens, We're doing what we've all done, deemed as the best possible thing given that there's no there's no perfect solution. Does it answer your question? Yeah. Okay. Yeah. You're kidding. Thanks, everybody.
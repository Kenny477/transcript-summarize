 You understand it possible to guess what's going to be on the test beforehand. It's very random questions. And right now, he's super Claire speak are very easy to understand and has literally been most generous curve I've ever seen in all of computer science. And as far as me, I'm somewhere in the middle as far as difficulty does. But I do have the most obscure video game references. I have that going for me. Okay, Enough about me. What am I going to talk about in this talk? Let's talk a little bit about what is architecture as my field? That's what I like to do. And I will give you the brief background on machine learning from an architect's perspective. So machine learning, machine learning person's perspective is as quite a bit different. But for me it means something. And then we'll talk about this kind of dichotomy between demo for responses. There's a machine-learning processors which have very, very different trends today. And what I think that means for the future of computing. Okay, so what is architecture? In a lot of different things, right? Didn't you might think of like the organization of the hardware building circuits, building chance, something else, right? And so I want to give you a sense of what computer architecture is me. And, and definitely feel free to enter either a set as before, but definitely feel free to interrupt me and somebody on the interesting, you know, if there's, if there's good questions and conversation and during that time. So architecture, I think it's easiest to define what architecture is. You can't look at systems that don't have an architecture, okay? And you can have a computer without having architecture. Actually, before 1964, computers basically didn't have an architecture in the conventional sense. So every time he built a computer, it required this tremendous effort on, on science. And so we have, first of all, you had to bone hardware. That's part of the machine that has some physical aspect to it. And then of course he still had to bow software. So the partners in that, it doesn't have the masses. Maybe you're, you're reconfiguring some wires and so on. The real problem here with these early computers to set the boundary between hardware and software was very fuzzy, right? So there was no real definition of, of what, you know, of any sort of abstraction can build on to write software. So heavy new instruction that you wanted to add that required filled with powder, which is very horrible. And there's no nose down into the interface to specify instructions. And so the consequence of this was that every time you wanted to build a new machine, you had to rewrite all of your software. A map. That was crazy, right? And that'll be it totally unimaginable today. Imagine now Intel purchases of new processor and then you'd have to rewrite everything to be able to use it. It's not going to happen. So one of the first machines that had this concept of architecture was any ATM machine and spam. This is built by academically. So he's a very famous computer architects, but done by numb. It gets a lot of the credit because he was Alon who was kind of their popularizing the idea of the ISA that publishing that the concept WE instruction set architecture which is the conventional or ketchup know today, ****** it. But I think it's a lot of credit even though he didn't really invent the concept of architecture. Because I want to kind of explain these trade-offs visually, I think will be helpful. And so we have the software world and the hardware road. And I think it's kind of, it's useful to think about the different layers of abstraction in these two worlds. And where architecture fits into these layers of abstraction has a very special place. And so if you think about like, what are the layers of abstraction in hardware? But there's the kind of build things all the way up from the bottom. And of course, you'd have to build your chips. I'm just something. You build them out of. Tiny devices, transistors. You build circuit some of those transistors and build those into components. And then you build those into logical pieces that connect together to do something useful. And by having these different layers that need, you can kinda innovate. And each one of these different layer two Ethernet, you can make a new circuit that is inefficient that at something and you know, your whole design will still work. And the same thing is true in the staff room role as you're probably familiar with or anything. That's top layer you, how the algorithm is, kind of what you want to do. You encode that in some application, you write that in a programming language. You have a compiler that takes that programming language and then pass that down into program, the operating system manages it. And so you can make a change. And each one of these layers and benefits, of course, that you are able to do that with. To change anything else? Okay, So before architecture, you might have, you have all these layers, right? And then you have all sorts of different machine. They're optimized for different things. So one machine might be better for big data workloads me when she was better for gaming, when machines better for mobile applications, right? That wasn't true at the back-end, but good. Because so imagine that being true today, right? And so if you wanted to, if you wanted to write all the different software for all these machines, is how to write it over and over and over again, which is very painful. So what architecture allows you to do? This is the interface between the hardware world and the software world. So you might call this as either the architecture layer or the instruction set architectures and what it's known as today. And this layer allows you to have different implementations of the Tiber that are all implementing that same architecture or ISA and all of your software can run on that. So this allows us to continue to innovate in the hardware and without having to change your software. Okay, I think this is all very abstract. So let's try to be more specific and I want to try to get you to be involved in this. So I want you to help me figure out what should be in an ISA. Okay? So I want to Design and Instruction Set Architecture. And I basically want you to tell me what are the basic ingredients that are going to be in that ISA. This exercise, either it goes really parlay or really well, depending on if no one is paying attention or not. But I'll give you a little hint here. So the basic thing that you need and any ISA in a has to be enough to run any program in any language, right? So if you imagine you have some C code over here in the lab, your Python, right? You have any sort of programming languages you can imagine. And you have to be able to run that on your computer. So what that means is that the kind of primitives that are going to be there in the instruction set architecture have to be simple enough that you could put them together and run any kind of any kind of program. And anyway, okay. So that's kind of can, and you can also imagine the kind of things that are inside of these lines. Just like there are different operations that you might want to perform as places where do you want to store some information? You kind of, you know, kinda execute the program one line at a time, right? So there are a lot of similarities between languages and, and those kind of have to be reflected in the ISA, right? So can anyone help me? Can you imagine maybe one simple piece that has to be there in any ISA? Okay, anyway, unattached, I look understand basic arithmetic, starring and reading. Okay, so we got a lot of great things, right in this one, This one answer, right? So we've got definitely, there's arithmetic here, right? You definitely want to have kind of operations that let us do something useful. And there's also memory, right? I mean, have, you know, they want some places to store information and be able to transform that information is something that we care about. So let's, let's at least the first couple of ingredients down. So memory, right, we want to have a place to store values, eight and variables. And we're going to also need some sort of instructions we already have that alluded here, right? So what is an instruction will just moves the state of our memory from, from, from one state to the next step. And this is all that has to be done in a deterministic way. Okay. But what about the what next fight? I have memory, I have some instructions that let me do something. What else do I need? As this as this enough, but I need anything else? While they are There's a lot of system stop fade those line. So that's, that's a good point there. So you won't be able to have a way to compile your programs. But I would say that we, we need more than just an instruction like we need a a definition of what a program as rights is that they said maybe a set of instructions. And we're going to put that in our, in our memory somewhere. But we also need a way to decide how to execute that program. Just have a bunch of instructions about how you decide when to execute each instruction that you have. And that's going to, let's go out to something I'll call the execution model. And so the conventional execution model that we use today is called Von Neumann execution. And all that really means is that it's a lot like when you're executing a program one line at a time, you execute the first statement, the second statement or a statement that's kinda like a logical pointer to where you are in the program. And vinyl and execution means the same thing. There's a, you're currently executing a, some very primitive instruction in our retirement program. And you will always execute. Next instruction after it next, unless otherwise defined by the program. So that's basically what, that's basically what an ISA, a lot of conventional ISAs are. And let's look, kind of look at how this, how does this get us? Well, the hardware will use this ISA to implement a machine. This machine is just going to, for our purposes, right? Our computer. It's just a machine that executes instructions one at a time and lives a little bit more complicated than that. At least that gets us most of the way there. Okay, So there's just a few basic steps to executing any instruction. Basically, we fetch the instruction biggest, get the instruction from memory, is being stored in memory. We decode the instruction, you just interpret what that instruction is doing. So figure out what to do. Maybe reads, the inputs are valid instruction, figure out what operation we want one, and we just execute that instruction and then we update the state. So it's basically like four steps to executing any instruction. And it's going to turn out that the design of the hardware is literally at implementing these four steps in sequence. You fetch an instruction with effects unit and you grab it, grab that unstructured movement, and then decoded instruction with the geek bug in it kinda bear, but the instructor does it execute that instruction with a small number of functional units. Those are the things that do, things like add and multiplication and load and store for access in your memory. And then you have a stage that updates the results by writing results back. That's basically what a CPU and this is a semi realistic, honestly, a semi realistic diagram. And it's, it's kind of close to what a CPUs because that's fundamentally what you have to do. You just have to execute the instructions from your program and in the order that was determined by the probe. Right. And then we can take his book Ss step back and look at like what does a computer system look like overall, now that we know what the computer, what is architecture? What is it? Wonderful. Okay, First of all, I just want to mentioned that there are a bunch of different instruction set architectures. So there are things like X86. X86 is the instruction set architecture that unless you're using the latest M1 Apple laptop, X86 will be the ISA that is in your laptop or a desktop was cheating if you have a desktop machine. Or basically these are the, the ISA that are most servers. Dangling. Arm is their arm is for, you, mostly use for mobile computing. And there are other ISAs that basically no one uses, mostly our next 36. Okay? So these are the, you know, the instruction set architectures that exists, right? And so in the software world, part of this ISA has kept us while you compile your program down into representation where it's just written in terms of sequence on these small primitive instructions. Okay? And a, basically a binary as just a program represented in those, the instructions in the highway, right? You have, this is a picture of a CPU and it is just implementing this logical diagram that I showed you when you have a right to basically executing its instructions in sequence. And then kind of finishing the picture of our system are our CPU can execute more than one program at a time. So here's a few different programs that are aligning at the same time. And our operating system is going to be managing the execution of those programs, which decides which program runs at which time. Okay? So this is like overall view of computer systems. I says a run around for a long time. And this is the diagram of the different processors and Intel has made over the years. X86 that has really been at least the most successful ISA for the longest period of time. And they're probably more ARM processors today. Then next elicit pastors. But at least, you know, historically X86 has really been the kingdom. Isa. Intel made this, I say back in the early, mid seventies, and it has been around ever since. So these are the difference of different microprocessors over these years. And what's amazing is that first of all, be on, the complexity of these science has increased exponentially over the years, right? So if you take a number of primitive components, the number of transistors that are there. And the most recent Ice like design that is in the billions of transistors. Whereas original ADA design in 1974 is going to be something on the order of thousands. So businesses, many orders of magnitude difference in complexity. However, because you have this This abstraction layer, a program that you wrote in 1978, basically can still run on a processor that you made a couple of years ago. So like 40, 40 years across their improvement. And you can still run more or less the same program on your checkings. Kind of amazed. And that is why that takes six ISA has been so successful because your software can kind of come along for free without having to really do anything. And that's, that's basically been Intel's business model of the medallion course. There's other companies making processors, I guess. As AMD here, making its own versions of vaccines, sags. Tell me about the things that are not X86, but at least this is in part of the story. Okay? So to summarize this part of the talk, I say attracts a hardware, makes a software stack simpler to manage over different hybrid designs. Von Neumann ISA is used by every CPU, your own, watch, your cell phone, your laptop, basically the Cloud, right? They all use really simple by knowing ISAs. My program is specified as a set of instructions. And execution model is you just execute one instruction after the other, really released. And you now should know basically how a computer works, right? We just execute one instruction at a time in this pipeline. Okay, before I go onto the next part of the time because there any any other questions? Anything you want to ask about? Am I making any sense? So okay. Alright, so well well, we'll keep doing them, but again, feel free to interrupt me. Okay, We have a question. Great, Perfect. So what's the difference between R and X 86? So it's a, a, a complicated question. But I'll, I'll give you the high level answers. 1986 was designed kind of a little bit earlier than arm. X86 was designed with. It was designed to suddenly Israel. It was designed at a time when memory was really, really expensive, right? And so the most important design constraint for actually six was that you, you, your program to take the least amount of space possible. Okay? So there's a lot of optimizations in X86 to keep your instructions really small. So that's one thing that's, that's one thing that's there. And actually six, the other part of that is a little bit unique, sex, and it's been around for so long, has so many editions. So there's so many little expansions here and there. They increase the capabilities of it. Add instructions, right? So they make the, the amount of memory that you can address more. And because it's gone to so many different iterations of design, it's very, it's very ugly to actually six just has a lot of, you know, it's kind of something that's kind of like something that you you patched a whole bunch of times. And and it's just, it's just not a very beautiful design, even though it's a very effective bias. Hey, an arm is just a little bit more designed with a little bit of experience in mind. And so it tends to be a little bit simpler. And that, that's, that's nice. It doesn't really affect the efficiency of the designs that you make with them. But it does affects the, the readability of the assembly code at the ISA level. Are there any desktops and laptops? It is arm, yes. The M1 new M1 from Apple uses arm. Is there anything you can compare? And I say to you like an everyday objects. And I say, I say just like a language, right? It's like a language that you can use to communicate. But the language is very simple. It's like it's, it's like a language that was designed so that you so that it's very clear what you mean, right? And it has to be simpler than a programming language because programming languages has all sorts of nice features to it. But they'll language, especially the simplest thing I can think of. We'll talk about what you're going to replaced by Neumann architectures by I will, okay, Well the one more question. What's the difference between the H event and 64-bit essays? That's a great question. You probably, if you've been around long enough, you've, if you've had the ability to buy both 32 bit and 64 bit X86 machines. But they're each a bit about what does it mean for something to be 32-bit? It means that the amount of memory that you can have is two to the 32 bits large. So two to the 32 bits happens to be about four gigabytes of memory. So if you want to have 64 gigabytes of memory, that means if you want to have more than four gigabytes of memory, but you need to have an eye a bit width that's higher. And so that's why in the 2000s. Both CPUs and desktop CPUs. And most CPUs I'll switch from 365 it ISA's because you just need to be able to address more memory. Yeah. That's right. Yeah. That's a great, great question. Okay. Cool. So I will get back to the top, but thank you for asking all the questions. All right, so now what I want to talk about is some, some trends and computing for two different kind of paradise that we are that are really important today. One is general purpose computing, which I've already alluded to and vinyl and then I say, the paradigm has been computing. But there are also processes that are designed for doing deep learning. And maybe you've heard of some of these pastors and I'll tell you a little bit about some of the companies that are building deep learning processors today. Okay, so I have to tell you what deep learning is and from a, from a computer architecture perspective, right? It for obvious to make sense or to be, to be relevant in some way. A definition and try sleeping United States. And I'm not a machine learning person. But this is kind of what machine learning is from, right? So basically I want to write a function and, but it was the functions too complicated for me to write it out directly. And instead, like for example, I want to recognize cats, men, and that's, that's really too hard for me to do. Okay, so I'm going to use data to train a function. And that, that data is maybe just the hope, in this case, write a whole bunch of ham labeled images of animals, plants. And so how is this going to work? Well, I'm going to define a form of function which is really easy to training. So it's going to be a really simple function, has a bunch of parameters. I can tune those parameters. So that hypothetically there's a way to train it to do what I wanted to do. In the case of deep learning, these functions are incredibly simple. Them most, most of the actual work that is done is in linear functions. So you're basically summing up the, the inputs times some weights. And you can have like layers and layers of these linear functions need something more interesting, right? And sort of trainees its functions you to gently nudge the parameters towards providing the cracking. And there's just more complicated ways that you need to learn, be able to do this. But at least from a, from an architecture perspective, that's basically what's coming up. All right, so this is the best cat recognizing the best image, recognizing neural network back from 2012. And it basically has that and this is what it looks like. So there are things called a convolutional layers. You want to see five, it looks like vegetal layers, right? But what is this at the end of the day? Well, if you kind of pick, I have a pointer. If you pick one of these, hopefully you can see my mouse. Just like 1 in the middle of this volume. What is it doing? One of these little points is called an activation, let's say one element or one of these volumes, my call the neuron. And that neuron has connections to previous layers. So this particular neuron has some connections to those with previous layer. Here. My horrible drawing. And, and, and the weights of those connections are called weights are called synapses. And so this is another networks, basically transfer visualization of the transformer network. This isn't most admired, most recent, being similar to what's in the most recent GPT-3 language model that's used by Google for all sorts of paths, right? And if you look inside of, it, doesn't matter what's going on here, right? This, but it does a very similar kind of a principle, right? Where there's basically an activation. It's activation is dependent on some of the activations from the previous layer. That's just as good as I'm going at it. Okay, so what does each of these activations are Nye's doing while you can think about it. Like it's basically just fitting data. So if you kind of fit data over and over and over again. So each activation is a linear function of its inputs. So you can take your inputs, you multiply by the weights to get the output. And more or less the, the weights are but the slope of the line that you're trying to set. And training, right, is just the regression of the solutes, this line to the data that you have. This is a very high level and abstract overview, but hopefully that gives you a sense of what deep learning is, at least from, from my kind of perspective. So computationally, what does that mean for us? Well, computationally, all of machine learning is basically boils down to doing multiply accumulate operations. So you have some inputs, you multiply them by some weights, and then you sum them together. And maybe doesn't like small nonlinear function that you asked me to come and occupational function. But the vast majority of the computation happens in this heavier in this regime here, when we're doing the multiplication by the weights. And you can actually take that, takes observation one step forward. Maybe you've seen diagrams that look like this. This is like a really simplified diagram of a neural network. So each one of these little things are neurons. These little connections are the weights between the neurons. And each one of these neurons is just doing a multiply accumulate right? Now if you kind of look at it, if you were to like write down all the computation since you are doing, what you will find out is that you're basically just doing matrix multiply to take a bunch of, bunch of weights, multiply them by your neurons are activations. And you get the next set of activations. And it's basically just matrix multiply. The conclusion here is that pharmacy at foreign computer architect, deep learning is basically just linear algebra. And this is really useful because regular computer, CPU has to be able to do anything, has to be able to execute any program that you like in any programming language. But a deep learning architecture really only has to be able to do things like convolution and matrix multiply really fast. So that's kind of fundamental difference in what these two kinds of architectures need to be able to do. Okay, So a lot of folks recognize this and the potential for making really specialized hardware that really, that only does linear algebra very efficiently. And one of the groups of people that were able to recognize this pretty early on Google. So back in about 2014, Google wanted, basically wanted to be able to do speech recognition based search. Me. And not everybody has speech, speech based. Hey, Google, IBM and so on, right? But they got basically back in 2014, Google calculated that if even like a small percentage of the search queries, we're going to be spaced that there very quickly going to run out of computational power to be able to serve all those crates. Now I didn't want to have to buy a bunch of GPUs or other kind of computational hardware. And they decided that the best route was to design it themselves. So designed for TPU processor. And, and U is basically a big matrix multiply machine. And this was kind of crazy, right? This says, is untrusted. Google as a software company, they don't build hardware, but they, they did it anyways, a higher team of people to do it. And it was, this is like, yeah, basically unprecedented for a software company to do. This is a complex view of what TPU is and I think basically don't want to describe anything what's in here. Hi, it's really simplify down. It looks something like this. You will also some memory into long-term memory from an anterior shapes. Many do a matrix multiply and you can get the values anyway it back, right? And, uh, you know, you still have to execute is these matrix multiplies one instruction at a time. So if you were to compare this with the CPU, it's, it's kind of like a CPU, except that instead of doing one tiny little instruction at a time, you're doing one matrix multiply at a time. And so it's, it's gets really at the end of the day, like these machine-learning processors end up being very simple. Lot simpler than you might imagine. Okay? So I want to now discuss it has a base that's the backs and I have some background on CBS. He has some background on deep learning process or just a tiny bit, I want to tell you what's been happening over the last seven years or so with deep-learning processors and CPU processors. Okay, Anymore questions before I go on? Now, okay, cool. So let's start off, but let me just tell you what's been going on with deep-learning fastest. That's right. So first of all, deep learning has had a profound impact on academia. So like Intel was doing SRA, Google is doing its thing back in 2014, meanwhile, and they didn't tell anyone about it for several years, so it's a big secret. Meanwhile, other academics, we're basically doing the same thing. So in 2014, this group of people from from India and France and then also ICT in China because the name of the university about this architecture called Diana and I know was what kinda the first machine learning architecture paper that had a really big impact on the community. And this is the design of this, that this machine, it doesn't really matter what the design is. Basically it's just a big dot, it just does the vector dot products. So not too surprising, right? You can take vector dot products and make a matrix multiply out of that. But it was a hundred, two hundred, twenty times faster than the traditional CPU. And it uses 20 x less energy, or at least CPUs at the time. And this is kinda like is amazing for two reasons. One, it's amazing because it gives you all of these benefits, right? So this is like way, way, way faster than what you've been, have the CPU because it's only doing one very specific task. But it was also amazing because it was so simple. In this design is like, you know, something bags. My grad student compelled and, you know, a few a few weeks. This is not yeah. So that was kind of a big wake-up call for architects that you can do something really, can do something really simple and give us really important, really powerful accelerator on it. And so they just started to inspire a lot of people to start building these kind of deep learning accelerators. And so I won't go into too many details. But like this, this, this paper and a few other papers and started this massive snowball of research into deep learning computers, right? So including some designs here at UCLA and above, FPGA, CNNs. Before I got here. A lot of really cool designs. 2016. So these are all the different papers that were published for many years, right? So just as one example of Isaac paper, it actually does machine learning using analog arithmetic, right? So instead of using a complex multiplier, you can use like a single transistor, analog transistors, you do matrix multiplies and you can bug, increase the computational intensity by orders of magnitude. Yeah, you introduce this concept of sparsity, right? So if you do sparse matrix multiplication, then you can skip all of the computation with zeros. And so you can also, you reduce the amount of space you have and stuff you have to star, and also reducing the computation that you have by orders of magnitude. Keep dead. Google finally came out and told the world that they were doing in 2017. And yeah, I could keep going. But basically this, but the point here is that this, there's this explosion of research in hardware architectures. And just as a kind of give you a summary of like, this is not just like a bunch of redundant research that, that isn't meaningful, right? That is made, we made huge advancements in computational capability of design. So this one picture kind of gives you a feel for what is the computational, basically improvements in computation intensity and energy efficiency of the designs over time. So this Irish paper, I think, was released, I want to say in 2016, February, right? And in the last five years, we've made designs that are over four orders of magnitude faster. So what does that 10 thousand times banks share and a 100 more than a 100 times more energy efficient, right? So this kind of gives you a sense of the order of magnitude computational capability that has been accomplished in the last, just a few years. It's not just an academic thing. There's been this kind of crazy explosion of investment into deep learning technology, right? That's billions and billions of dollars of funding for machine-learning hardware. So this is just a few examples of some of the designs that are there in big tech companies across and videos there. They're building their specializing, They're used for machine learning. So the building very custom specific processors for that are there in their GPUs and do deep learning. Of course you have TPU, Microsoft has brainwave, Amazon has inferential, yeah, Baidu has Comune. Apple has some neural engine that's in there, M1. Facebook or Meta. Now at now has, has some secret stuff they're doing, but they're also building machine learning processes. It's actually not a big secret right there, hiring the deep-learning hardware engineering. So yeah, basically all the major and this is very strange, right? All the major software companies days and accompanies are building their own deep-learning hardware, which is just crazy. And what's even more crazy is the number of companies that are building their startup companies that are building their own hardware. So this is, this is a list of some of the, the hardware startups that are now very, they're successful in some sense. At least they're successful in raising money and building chips. As far as getting their designs into entities. That's another story. But at least there was this huge explosion of these different chips. So this is just, you can see some of the chimps here. I guess want to, maybe one of the coolest ones to look at, at least as the cerebrum. And so this chip, as you can imagine, a normal checkers about this bank, right? A couple inches on each side. The Sabbath ship is like 10 inches on a side is basically a 100 processors all stitched together and that less than communicate very fast. There's a lot of other things I could talk about here, but I'll probably skip over it for time unless you want to ask more detail. But this is the, the Battle of the battleground right now for AI startups. And over here I'm listing the different startups, so the amount of funding that they've got so far. So we can see that the funding is basically, I'll just venture capital funding. And this is an incredible amount into the billions of dollars of funding for these companies. And it's kinda the gain here. Companies are playing as they are trying to make, kinda make a shape and trying to get acquired. And Intel is kind of the main company that has acquiring these machine learning companies, hotels acquired four of these so far, Not all these acquisitions were particularly successful, but MittMedia, a couple of them aren't. And yes, some companies get acquired to do very, very well. All right, so basically the game is be successful. Get acquired, are about a Chapter 11 bankruptcy. And this is, this is what happened to waive computing. And there are going to be many, many more companies that fall into this bucket is a brutal battleground. There's only going to be a few companies that survive. But it is really kind of exciting and crazy time for machine-learning hardware. Okay? So that is the story for like what's been going on in machine learning hybrid. Very, very successful, at least so far. But whatever the rest of computing, right, There's like all sorts of different kinds of areas that we care about for computing. The things that we like the gaming. But there's also like all sorts of really useful things of any processing web services, scientific computing and graph analytics, image processing, right? So what about Ali's for areas? So just like I talked about like what's been happening with the research and machine learning. Let's look at what's been happening in research for CPU architectures. And so this is seven years ago at the same time, Diana Prince out. This is the very most advanced CPU that was there, is Intel has low. And you can kind of take a look at Inside. Intel has well, and I just want to say it and I'm like a computer architect named, but like to take my word for it that this is an incredibly beautiful design, right? There's the purple part of this design is like trying to like fetching multiply. Basically like fetching multiple infections that time. But it's, it's predicting what you're, what the program is going to do even before the program does it so ill no. Like what you're afraid access to three times they'll know like what direction the if statements and so on are going to be. It's crazy, right? And then this part of the pipeline is like been reading like tens of operands toggles, same time, trying to figure out like what is the best border to do all the operations in and is re-ordering this product pipeline. It's going to reorder and schedule all the instructions. So like there could be some instruction that's like way, way, way further down in the program, you're going to do it now. Then this part of the pipeline is like executing tens to hundreds of instructions all at the same time. And then this part of the pipeline, it's comforting, safe remembering like executing What's a memory instructions at the same time. So there's like lots of really crazy things that are in this design that I don't have time to talk about, but it's very, very, it's a very elegant sign represents the accumulation of research over the last 30 to 40 here. So it's not entirely really the design of the cool stuff here. It's like Intel plus all the research that's going on. In best processor 2021, Apple M1, firestorm pastor, it was this little, little piece of the M1 here which looks like a performance cores of the M line. This is kind of performance can arguably right now. And if you look inside events, this is basically what it is. Take the old design and then you rename and Firestone. Okay, I'm leaning a little bit facetious here, but this is more or less true that this, this amazing new processor or whatever m1, it's, it feels amazing because it's just a little bit better than the previous design. Maybe to x over though in performance without thinking about frequency over the last seven years based not, not nearly the same kind of benefit that we're getting in machine learning design space, AI and machine learning architectures. This is a more accurate picture of what's happening in an app, a firestorm. What it's really, it looks very much the same. Small tweaks on the same. Okay? So this is the paradox, a general-purpose processor architecture. The performance improvement is stagnating while machine learning process or they're frightening. And there's two reasons that this is happening. Then I'm going to explain. One reason is that there's just no longer this technology fee, right? So it used to be that like you you're sorry, that you felt less plastic gear, you build it in a new technology and knowledge, test it smaller and faster and uses of us. I know it's just not show any line. And it's also, it's really hard to improve general-purpose microarchitecture. It's just very, they've already extracted all the fonts that you can out of general purpose architecture. Here I want to explain the SEC, not that this first kind of reason. Okay, So this is what I'm going to, I'm hiding a little bit of this graph over here on the right this second. But basically what this graph shows is for over the last five decades or so, for each one of the, I think these are mostly CPU designs. What are they? What are their characteristics in terms of the number of transistors that they use. Performance, that's the frequency and what's their power. And on the y-axis, this is the Avon scale access services. So linear changes on the y-axis are really exponential improvements. Okay? So the first thing to say is that since 1972 now the number of transistors as increased exponentially. And he says, I mentioned that earlier, but also a single threaded performance. The frequency and the power of these designs also have exponentially increased. Okay? So this was like from 1978, 2005 or so was like the golden era of CPMs right near your CPU, just getting exponentially faster and cheaper. And that was, it was a great time, right? And and everything was was just looking at the time 2 thousand or does it by like we're just kind of like driving off with but I didn't realize that there was a big problem ahead. And what happened was the kind of reached a certain point. You can't just keep increasing the amount of frequency. And because basic basically or power limited, right? So you can't, you can't just keep going and making them faster and conventional way doesn't work, right? So if you, for the longest time, Do you think back like ten years earlier from now, if you're buying computers back then the frequency is basically that you are buying is basically the same. And that means that the improvements, it's a single thread performance, much, much less than they used to be. And this only shows up to like 2018 and it's even worse today. Okay, I see some sort of question. Let me look at it. Alright, so the overall thing here is that the kind of the scaling trends are just no longer favorable to just getting, getting performance for free. So always asking this question, we'll consumer electronics start to adapt and our past certainly the getchar or our general computing clusters, irreplaceable and space. Great, great, great question. So I would say that general contributors are irreplaceable. First of all them, right? So the kind of computer that you need to run, Microsoft Word or aren't around, Overwatch or whatever on your computer. It's going to be as if you if you need something really general to be able to run it, anything we want and not, and not everything can be converted into a machine learning or linear algebra type computation. So CPUs are going to be around forever. I'll say two things. One thing is sad. There definitely will be a question of what are CPU's will get better in the future. And so they've been really statement for about five years. And so there's a question of like, how do you can you, can you change things to make seeking spire? I'd say. And I'll talk about that again in a second. The other question is, now the other thing to say is this that will, well consumer electronics or to adopt an all processors definitely, right? So, you know, Apple has put a neural engine in there and launch chefs. You have Intel, Qualcomm, Samsung, they're all doing the same thing. They're all in a latest, the latest Google Panther has kind of a small version of the TPU inside of it. And so if you can find things that look like when your algebra or you just have some machine learning applications, then you can put those computations on a machine learning processor. So not everything will be able to fit there, but only a certain set of things that well. And there's enough of those things that you'll definitely see in machine learning process tears become kind of the standard, one of the standard units. That is in any design. That doesn't mean that it's replacing a siege. Next ask is, are other, other types just specific processors in developments? Or is it just an hour processors? And that's such a great question and I'm so happy you asked the question, right. So that's actually what we want are the things that we're looking at is, and I'm back. I might research group is looking at tobacco. There's all sorts of other applications that you can focus on rate. And so one of the things that my group does as we look at different really important problems. A graph analytics or video passing a single thing. And we build accelerators that are kind of good for these different domains. There's a lot of research going on in other domains. The challenges that each one of these domains is just not as it doesn't have the weight behind it, is that machine learning has an economic force that machine learning has like. So it's, it's a lot harder to, to get Google to adapt a graph analytics faster for them. So Jeffery asked, where does quantum computers finance picture? I'm not really quantum computing person. But I would say for, for the mean, for right now in the short-term, quantum computing is very difficult to do. In a commodity processor. You need very specific conditions. Even though they're like certain computations that are really fast and they're not really going to be on phones at least for the foreseeable future. And so to me, that's that's a big difference in what, you know, what I would pay attention to as far as like conventional computing. But the betas are another kind, an accelerator I think celebrate certain class of applications. All right. One last thing I have, like one more slides ago. So he's going to talk about a little bit about why machine learning processors are so successful. And basically I would just want to summarize this isn't and we'll kind of cheating. Okay? So remember like I showed you this picture of a CPU RAM, the CPU general-purpose processors. And there's this layer. And the middle, that's the ISA, right, is general purpose architecture layer between the software and the hardware. And that layer lets you write general-purpose applications and have them be executed by the hardware. So what is going on in these machine learning systems, but they're a little bit different in the sense that the, this layer between the hardware and software is extremely thin. Hey, it's been in the sense that they only have abstractions for very specific operations. So they only have attractions for like matrix multiply convolution if you have a linear algebra operations. And because that layer is so thin, it's very easy to co-design class later say, you can, your compiler now, for example, right? Can be specific to linear algebra. You can have a linear algebra compiling. Your programming language doesn't have to specify arbitrary any procedures and function calls and recursion and whatever, right? It only has to do matrix operate. So you have a very specific programming language that really only focuses on matrix operations and your hardware also, right? Ethan also cutters. So you can say your hardware is, it's only going to be able to do dot products. You can even design at the circuit level where you can say, you can build like little analog transistors that do multiplication complications. So you can, it's very easy because this layer is so thin, you can do co-design across all of these layers. And that is a lot more difficult over here, right? It can't, it's very hard to make it. But what do you make your compiler is specific to. What do you make a programming language specific to, to make it more efficient. So not really possible in the same line. Okay, so that is the base peak to the conclusion, right? So wow, does this mean the end of computer architecture? And it's just, right now, it just looks really hard to use the instruction level attractions from conventional Dynamic DNA and make computers faster. But what I think we can do is we can learn from, I think there's hope so. I mean, I don't think computer architectures over. I think we can learn from what machine learning, such aren't doing. Some machine learning pastors, they come cheated, did use a very thin ISA. But we can take the principles that we learned for machine learning in clusters. And I think what we need to do is you need to design, ISA said are what I would call co-design friendly. So they co-design friendly ISAs need to be able to expose more of what the underlying hardware is doing. So they need to be less abstract, so to speak. And so that's a lot, a lot of my research is about is designing new kinds of ISS that light you do, co-design it in the same. I think it's a little bit abstract, but I will I will leave it there and take any other any other questions? Okay, I see a couple of questions. One is from Rory. So yes, since modern pastors contain billions of transistors, how are they physically built both on an individual level and in mass production? That's a really good question, and that's a very, very broad question. I'm so parsers are designed also with a lot of layers. This is kind of a tangent, mistyped, but their design of a lot of layers of abstraction, right? So when you design a parser, you're not designing it out of transistors. And I'm building like your designer little transistor as you're building it out of components. Okay? So first of all, kind of side how many cores to you on how big you want your story structures to be. Teed you like is the kind of things that figure things out, the design level. And then you will construct the design that you write simulators, right? And then construct the design out of. However, languages like RTL and RTL is you have to worry about transistors and it's kind of abstract that away from you. And then that gets processed by you slowly lower that complex design down into physical design my errors, right? And there's a lot of tools that help you do that. So it's so it's not as yeah, it's not as hard as it is maybe making it sound. Our ML processors compatible with the X86 architecture. Yes, in some sense, right in and I would say. The way that you interact between AML processors and actually very coarse-grain. So you might have like a function call right from your normal program it says do matrix multiply. And then a function call will get translated by the hardware and then been executed by the machine learning classes. But some, it doesn't the interface between I'm Alan and an X86. It doesn't need to be very, it doesn't need to be a very tight interface and a loosely connected interface to that. So that keeps things simple, but it's before. The question is, can you run X86 and an MO faster than no need to not run? You cannot run a conventional program on processor. Jeff. Jeff, he says, Could you elaborate a little bit on what co-design beads? Yeah, great. Yeah. So I think what I'll do, I mean by co-design sets, you are, you're designing two things together, right? So you are thinking, normally, buy conventional design. You're designing one layer at a time. So like X9, the compiler, I just think about making the compiler faster. So that's conventional design. Or I'm designing some circuit and I just think about making that circuit a little bit faster or a little bit more energy efficient. And co-designing means that I'm looking at more than one layer at a time by looking at the compiler and the ISA the same time, I'm saying, okay, I need my ISA to expose some information to my compiler. And I can also design my compiler to be able to take advantage of that. And so it allows you to do optimizations that wouldn't otherwise be possible. That answers that question. Let's look at rent, especially Amanda says imperative programming is kind of similar to by nine. And I say, Is there any research into declarative paradigm inspired ISAs? I'm sure there is. I'm not an expert on that topic. Most ISAs are our very imperative. And if you think about it like it's much easier to design your ISA to be imperative because you're basically telling your hardware what to do, right? You want to be able to tell you specifically what to do and you ought to have control over it. Whereas if you're just like declaring that a certain amount of work to be done, then you're kind of relinquishing control. So I yeah, I don't know of any mainstream processors that is declarative ISAs. But I would be surprised if there wasn't any sort of research and literature on that. I think it's an interesting question, pressure. All right. Anything else? I guess we're pretty much out of time. All right. Well, I will I will I'll I'll leave it here tonight. Keep you guys longer. But yeah, thanks for listening. And yeah. I mean, this is amazing. Yeah. Afraid to come to my office hours and chats and time and this is interesting to you. Okay. Take care everyone.
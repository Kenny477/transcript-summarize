 Okay, I might go up. Okay. So again, l egophony, Computer Science Chair supervising the class. The, the real soul behind, behind the glass is TA January zag. And she landed a citation that we some obligation includes in the class. There is no need to be present synchronous. In other words, the right. Now in the class I'm not thinking presses or anything like this. But there is a requirement to have to be present in the recitation because that will be given online in time, quizzed. And we don't want to, you know, to, to diff different times of quizzes or anything like this. So presence into a situation is necessary. This class is about one hour, one hour a week. In each hour, we will have another professor or guess representing some area and introducing you in the most, you know, assuming that you knew already what is programming and stuff like that, what to expect in about two years when you become a junior and see what areas. Once the exciting stuff. For example, I will not speak because my area is not that exciting, I think. So I was not to bother you with that. And I also only give only graduate graduate class. So anyway, so that's it. So all the class size, there is a class site in CCLE. You'll come to the and the PA. If you don't know how to reach there, We'll get there. The TA will explain to you the citation. And the this is the class site is where the taping is posted. So that if you don't, if you if you didn't, if you will note that in real-time, you can later a portrait. So today I want to introduce today we have a speaker, I will right now, probably a guest. Professor, Zhang from Hong Kong bought a tempo. I mean, soon it will be professor in our department joining us in January. And he will talk today is one of our next power. We are very, very advanced, considered one of the best departments in machine in deep learning, machine slash mark machine learning. And he will be an addition because a marble, the Machine Learning, he's the one that is the only one among them that did some work in the exciting area of self-driving cars, which I, which I thinking it's a big future. So I don't want do it since we have just our will and we already 10 minutes into it. If you want to ask questions. We do it at nine o'clock. It's the forms all that wanted to stay behind and not any questions about the running of the class. Feel free if during the, even whether professor will lay egg laying on. But I really agree with it or not. You have the function of a and raise your hand. Raise your hand and he will long-term. Fetal, feel free to ask questions while the donkeys going. We don't have a set amount of material that we have to cover. And the list of what are the classes should be if you're not yet on CCLE. So I'm passing the baton to bolide to give I'm less than five in the 15 minutes left. And universities stock. Yeah, let me first share my screen. Yeah. Of course you can share. Okay, great. So I guess everyone can see my slide on it. Okay. So thanks for that kind introduction, also science for that invitation. So I'm honored to be with you today for this brand new quarter. And gave this stuff, but her first guest lecture. So you know, you already, we have the most of students in the first few lectures than a tendency jobs exponentially. So let me first introduce myself and Bonnie Jo, I'm a faculty member now always the Chinese University of Hong Kong. So tied to route Vm, not UCLA yet. So you guys are earlier than me, but I will join already next year. So right now it's a Meta night's near Hong Kong, my time zone. So I'm giving you this lecture, crossed the Pacific Ocean. So please don't get mad at me by systems and crazy. Okay, So I'm doing research in artificial intelligence with a focus on computer vision and machine are part of me. I'm going to give you a brief introduction on human centric AI, a topic I'm very excited about. So feel free to stop me and ask me a question. Otherwise, I'll get the sleep for sleep. Um, so I guess we're all excited about the success of artificial intelligence. So we can see that fish intelligence, I've been widely used, in real words. Self-driving, to move by robots, be blocked by Boston Dynamics and to Google deep mites that can be blobs are superhuman AI that it's best human players on the game. A bot, or reasons the work, RPA foods that can predict protein structures as q max for you, however, are giving all the success. We all so see the challenge of the AI's. We see many car accidents made by the auricle pilots AS system. We also see the bios in the prediction skimming by some commercial AIs, a guess. Underrepresented groups. We also see that red pens of the fake videos and images. And that's called privacy and security issues. I think in creating online. But I think this is a good that you say, we've seen many accidents. And I would object and because I didn't, I don't have the statistics. But they only two for Tesla who is being investigated about it right now. Why is the rate of accidents for Tesla is 1 tenth of human beings? And there is a big question here, which is level, which I say to the students actually already in the home. And I met with the splitting for ACM and UPI groups that they will be in the future. A point of dye which we will have to say, if machine is, on the average, it is better than humans by a factor of 100, then release control and we sit in and let them know that there is some probability that nothing new, we did anything, nothing wrong, but that will be an exit then because this software of machine learning, we don't, and I don't think we really know how people do things with complete assurance that it will behave in subway correctly. We don't vilification produce software. Justice thing. Yeah, I think acoustical be speaking. So accident rates of AI actually is lower than the human accident rates. But that's still, we need to figure out why the machines make mistake here. So my argument is that we still need to figure out why the machine make this accident. We still need to investigate why do we make mistakes? So yeah, Yeah, Fiona make mistake by several ways, but for machines we still need to figure out on the yeah. The potential mistakes made by the machine. Okay. Yeah, That's continue. Either suggests that introduction slide. A lot of things to talk about today. So you know that being artificial intelligence, nowadays, the most popular AI models our deep neural network or deep learning models, because I'm working computer vision, so many deep learning models have been developed for visual recognition and the object detection, such as resonates, Masdar US and alone or random net. Also. On nature nine great processing still have this transformer or GPT-3 that, that have billions of parameters and the kingdom massive number of pathos than it can outperform all the previous machine learning models with large modulus. So in speech, since the signs, we have the permanent magnets that can even help you order the foods from the restaurants. So giving so many network and the performers have been greatly improved. So we're all excited about just deep learning models already match the human level performers. But before we become too excited about this, let's come back to the history. Let's go back to the 50s. So the founding father of artificial intelligence are entering. So he wants to propose fundamental questions in this on papers on consider the crushing come machines sink. So that is our fundamental question that motivates the AI researchers on to develop more and more intelligent machine. So behind this question, there is a key assumption. So the key assumption is that the machines shoot this scene look like us humans, you know, the machine shouldn't sink like a sponge or simple like a fish, it should sink as, as humans, in another words, we humans should figure out or understand the inner workings of those machines are so deep learning models. But you may come back to our people, work on it and save those models are complicated. To know what's going on, on insights, billions of parameters and hundreds of layers inside and trim down millions of Betas. So it's really hard to figure out what's going on the sides. So you will show an entering all of those models. We have. Calibri here will be puzzled about this models. So we still didn't answer the question is, can those AI models, I think like as humans, right? So for giving orders, difference, architectures and models, we actually, we can summarize the deep learning models. You will a simple diagram. We just tremendous models are massive number of data. No matter what pasties you will have, inputs and outputs. So you know, opportunities, you can just skip this model inputs. Then this model will give you amazing goods outputs prediction. So people in industry really like this type of diagram. You just trim the models on the massive number of computational resources than this model. You can just plugging units modal applications that can behave amazing well. But here we still have this fundamental question. So giving this superhuman model on Canvas people, yeah, model seemed like the humans. In another words, can we better understand what's going on inside this models? So giving this a grand challenge, this question can be banned model sink like film and we can ask more specific questions about this. For example, giving this outputs. So we can ask about why is this output? Because one person, although give you some prediction. So we want to understand why now this prediction, so he's about to expand and baby teeth of AI models. Also giving this an early representations massive number of data. So we want to ask about that interpretability is normally the representation wiser, that reputation is an interpretable. Whether we can visualize what's going on inside the model. Also, given the inputs, we should ask about candidates am modal handle all the cases. Because when we deploy this model in real words, so not only we want this model worked well on our testing sets, we want, also want this model to work well on all the cases that's in the real words. So he's about children's abilities of the AR model. So we want the AI model we have, It's generalizable for all the cases. So under those different questions, so we really want to move towards this human-centric AI. So here I define the human-centered AI as that's we, we should not only care about on the performers of AR model, we also need to care about all the human-centric poverties of AR model. So we want to understand has been improved. Documents into properties such as interpretability. Still rabbinically, I'm generalizability of the areas. The older the attribute That's Rayleigh buttons on here. Under the Properties Island eventually, yeah, Hold on. We also wants builds up really loud crash. I mean mu 0. Okay, So we also want to buttes this reliable interaction between humans and AI because we wanted to human can better understand the decision-making process. We also want to put a Q in that decision-making loop that can better interact with AI model we have. So under this human-centered AI, I will give you some examples, some, some project that I've been, I have done with my students. That's how can we make this AI more human-centric? So I'll show you three examples on. So examples are across different pass in machine learning from the image classification on to image generation and to machine autonomy. So how can we better build this expandable AI and the steerable AI and the generalizable AI for different applications. So let's start with our first example. So here we are looking at on deep AR model unchanged for image classification. So this is a model I changed when I was up PhD student at MIT. So always my for my divisor. So we build up this large skew on scene classification dataset called a places. So basis contains more than 10 million images. So after we collect two images, then we train some neural networks. So after, after the model, I also unlike damaged, so I think the stem are still working so you can upload the image that's in this demo. We'll return the prediction. So for this particular inputs image, so it sinks, this is on a 72 percent if these involve booze, which is a correct prediction. And so I just play rounds and the uploads on images to try how well it performed. So carried another image. Just meet shout outs on. I think that that's the PEP quotes are in Rhode Island's. So just so I upload the image and download this model, give me the prediction on 62 percent Beach, which is also a correct prediction. And so here another very interesting photo. So just a photo of my friends. So he's just sitting on the floor, right. So, so I upload this image onto the demo. So in other words, give me some bird we're prediction. So we think this image is up inside the moisture off gene. So with very high confidence. So it's a 5% competence, or you'd say inside that much arching. But I'm pretty sure my friend is not grossly or some x0 so high that paid. I don't know anything about martial arts. So you may have a guess. Why, why this model give, give me this prediction. You may think this is maybe the, the pose of the person on, or maybe it's a slippery floor, or maybe just the empty space. So to better figure out why, it'll give me this prediction. So we developed some measure called class occupation making. So it's a measure that we developed to spin the prediction given by that deep neural network. Basically this cabin measure can euthanize in prolonged activation of that representations and there's a relation to the final prediction. Now, we can generate a heatmap. Two highlights the most important regions relevant to the final prediction. So from this example, you can see that actually that bare feet make this am although scene, this is the moisture out. So it's a little bit counterintuitive, but still make lot of sense because the martial art James, People want We are any shoes. So this model just capture this. Bare feet are the most important evidence to give this prediction. So, so, so this master, how this master work. So here it's a little bit on technical detail. So I guess I can just keep this part. If you're interested, you can take a look at the papers. So basically what we are doing is that we utilize the internal activation of the features and their relations. The final prediction that we can utilize this on relations and the January, the hidden map onto highlights the important regions. So this method can be used to analyze the model prediction. So here is our result. We have that we applied to some video prediction model. So the upper left corner is the top one prediction from that video prediction. So, so we can, for each prediction we can generate a heatmap. So you can see on the attention on, of the model actually shifting around insides, the video that try to capture the most important habitants or regions relevant to the prediction. I think some students ask a question you can, can be used to improve the accuracy of the prediction. Yeah, that's a very interesting question. So I think people actually use the technique to improve accuracy of prediction. So after between this first model than they, they, they use the attention map to localize the important region than the crops, the input image than sentence cropped the image back into the model and generate the second prediction is more, is more likely to take a second to look at those important regions than you can improve the accuracy of the prediction. So this Nasser also have been used in many different fields, have been applied to those medical you begin domains have been used by researchers from Microsoft, Research or stem from machine learning group onto generates the prediction or generates data. Transactional data. For, for medical imaging model for mammogram classification or chest x-ray classification. So it can localize the deceased region relevant to the prediction. So it can help the medical doctor to analyze results given by the AI models. So that is the first arch for this. So we, we, we can use the very simple technique and their relation to the final prediction and the build some Explanation map for the image classification. So you may wonder why the second panel on units or representations can help us localize the important regions. So we have some other works that tried to visualize that in Parnell units, the knowledge representations. So we actually found out there's internal feature or convolution filters actually that try to detect a difference meaningful Canvas API inside the image. So here I visualize Unit 2. You can see those units are detecting different pattern. The first-line detect the trademark in the scenes and second, detect them. And then the third one detect traveling FAQ document face. So we also can further amplify the key internal units relevant to that final prediction. So here I show you is three units That's the most important to the living room classification. So do the units above so far, coffee table and the Firebase. So here is another results that we identify the units relevant to the restaurants classification. This is the table with detection unit or share the capillary then Fuji catchment. And so you can see this up neural networks actually north doing this a black box on prediction. So it actually build some knowledge inside its internal representations. I try to recognize the, the, the units on the concept relevant to the final prediction internally. So if we can discover through the internal knowledge that can be more expandable AI models. So here we have another frustrating. So what is the units referring to in our model? So here are the model. We use. Some gnarly model quotes deep convolutional neural network. So here I write Refer the units at the convolution futures. So each convolution Theatre have some parameters. They were north from the data. So here we just visualize each convolution futures. So I think in nature that deploring cause you may know more detail about the structure of the deep learning models. Yeah, so here we just try to identify some layers, and each layer have multiple unit units. So we just visualize the units at the some layers and the show that's at some layers now some unit emerge as object detectors. Then we can use this, the emergence object detectors and to January the hidden node. Okay, And so that is the first parts. That's a beauty, more expandable EI for classification. That's moving to the second parts. Now how can we do It's more steerable AI for EMI generation. So here we are looking at another on deBroglie model. So it's AR model people developed for image generation. So the model courts are generated by the virtual network shortly. That's again. So that's the reason to be recent five years, there is a rapid progress for EMI generation. So we're here least some of the images generated by a GAN models. So again, was first proposed in 2014. So after that, you can see the image quality as well as the diversity have been greatly improved over the years. So you can say reasons that we began and the StyleGAN be too, can synthesize the images in different shape and categories. So we will realistic even pokey amount you cannot differentiate or whether some image are generated by that. Yeah, it's a real image. So here is some other reason the work done by an open AI, which is a company in Silicon Valley on pioneering this deep learning techniques. So they built the text to image mode so people can just type some sentence. They got an arm chair in the shape of a candle. So it can generate those kind of avocados should also here for the input as a smell made of hot. So you can generate now made of hop, hop made up some snow on this. The result is amazing because before this result, you'd be just Google like our culture, you'll get nothing from Google search engine, which means this model really become recreating to know how to compose. The previous concepts are in Tunis out into some new concepts. That since they are modal, become very creative, you can create some new image, France or some previous concepts. So here we are looking at this AI model for image generation. So for this AI models of inputs is some random signal and outputs, It's just realistic image. So reaching this random vector, so the input just like 512 random vector. So we change this random vector as the inputs, the output would change. So here is another inputs, random actor to another outputs. So this pipeline, it's very easy to use. So you actually can get cloners and more with this model generates different image. However, this pipeline I still difficult to use. So why is that? Because a lot of the case, we would like to customize the outputs are many ones are different output like for this particular outputs, we may want so different lighting condition. We may, we may want a different view of the input image. So it's irrelevant to the durability of the generative model. So common, the image generation pi-pi balance supports the skydivers durability. So how can we improve or improve the stability of January model? How can we customize outputs are in the problem that we want to solve. So along with my students, so we developed on some pipelines that, and try to identify the causalities are in a latent space of this January the model. So for this image generation pipeline, so from the latent space to the image space, we have our extract some attribute space, right? So we can apply some image predictors and can see this image spacious and nature liking and tidy. Then we want to go back into the latent space and try to identify the cause. Effects are relations. So if you can identify this quadratic relations, now, we can control this image generation process in January, scale this model to generate the image we want. So how this pipeline work. So it's actually pretty simple. So starting from that latent space, we just generate a lot of random vectors. So here you see a blue dots is a random vector. So we sample from some random distribution, then we can, and so into the generator, so G, then it will generate a lot of images with different companies, right? So here, then we can further apply some off-the-shelf image predictors. Here, we just predicts our indoor lighting of the generated image. Down here we can treats the predicted labels as a sugar label for each one of the image and also each one about random vector. Now, now here we in that latent space, we can produce change some linear classifier on using this predictor label I should label. Then there is a force, very important step that we would like to also do some intervention in the latent space. So here is a step called the counterfactual verification. So basically what we're doing here is just to put some convention in the latent space using this and normally the boundaries that are observed, how much the output attributes change accordingly. So after this step, we can identify some Leighton boundaries in sizes latent space are relevant to this, the attributes. So here is our attributes boundary relevant to the indoor lighting. And so here we have two image. So there are bacteria if Z1 and Z2. So at the beginning you can see just a major lighting, right? So because we have this boundary associating with impo lighting. So what we can do is that we just push the random vector that Z1 and Z2 across this indoor lighting boundary. You can see that gradually the indoor lighting condition of the Fourier image is greatly improved. The lamp is turned on and also global lighting condition also changed by simply just push this a latent code across a boundary. So here I further show you some demo medium that we steer the generated model. So here I have four images. You can see that the lamp G image. So here I just play a trick that I can just start her on the light. So you can see here as we push the latent code across boundaries and lamb be just magically turn down and enjoy everything. Just light up. So you get a latent space. No downing be cap and devices on boundary controlled indoor lighting. We also can have multiple attribute boundaries is covered in the latent space, such as clouds, non-cloud, and the vegetation. Lung vegetation. Here I would like to show you more than when Williams here, I have four images. So unfortunately, the weather is not good so that the background seems pretty blank. By right now, we would like to add some clouds inside an image. So I just push the latent codes across the Cloud Foundry so you can see that the clouds are just automatically go out in the sky. And also for the second example. So I really like aids because you can see it is that you have some set color projected on the, on the Cloud on the building. So it's a really beautiful manipulate nation results. So here are some other without that, I had for beauties. Sudan, not so much green space here. So we want to add some vegetation insights that will just push the latent codes across the vegetation boundary than this. Different trees and the grass and the green space just grow out in the ceilings. And also for the first example, you can see this model. Even smart enough to put a sidewalk on top of the grass so people can walk around. So from this the supervision of image generation, you can see this, that model actually try to decompose the concepts in order to compose the whole image. So we can decompose concepts, then we can control this image generation process. And so here is another result I'm pretty excited about. And we can also change the scene view. For example, here we took some image, so recent view of the same. Now we can push this meeting hosts in the different view points. Now we can change the viewpoints of the image. So this result is pretty poor because when we first treat this on generative model, the training image, just the image download from the web to do the image as an individual images. So we don't have the corresponding images from the same, the same battles. So you just do the model itself are really figure out the 3D structure in parallelly in order to synthesize those to the image. So we have a question. So others generated image from real source of all that the, created by the machines. Here the image I show you are created by the machines. So those are the fake images. So we also have some words that can allow user to input their own image. Now we can do the manipulation. So that will involve some other technique called GAN or immersion, like leaving a real image we can inverts or how can we invert or into the latent? Some students ask about, can you change this to account for the shadows on the object? Yeah, that's also very interesting question. So, so we don't know this. So sometimes, so this is the emergent concepts. We don't have any labels. So sometimes you can see that some artifact there. So it's not like doing this manipulation perfectly. So I think you need more future. We'll walk through you further improve this results. We want to control Eric heights of concepts. So right now we only can control the concept that we have some labels or have some on Trinity, the model for. Later, I will show you some other works that we do, some unsupervised on discovery. Um, okay, so from this example, so you can see that we can pose out, we can close the loop for this image generation, can customize output and now control the generation process. So you may have a second thoughts. So wait a moment. So given this is normally the patients, so what are the individual units are doing? Because for the first project, I already show you that's for the image classification. The individual units are doing the object detection. So you may wonder what happened to this generative representations. So we do some further visualization and nevertheless is to understand the role of individual units. We also found out that the individual units actually control the synthesize of difference on discrete objects such as we have a tree since it's either sky things as either and gracias incisor too, can identify a distribution of all the concepts. And also those concepts are change across different layers. And those layers close to that, our outputs are more come before this style attribute manipulation. There's two layers at the middle layers are more like account for this objects. Since as I said, after we have done the by the individual units, control different attributes. We can have this very interesting, the second track, the image accident. So user can just brush in some regions. Then we can just turn around and probe on the unit or some regions. Now we can add it's the contents. So I think this demo still working, so we can go to this link called the cage and then play with the demo to do with this image manipulation. To have some questions here, at which point in the image generation pipeline in the latent space fit into the mode. Okay, So the label and the random vector is a feat that beginning layers. But there are some reason. A model that's the feedstock latent vector at each one, the layer model called the StyleGAN, to actually have the random vector field. It's Abby, each one of the layers. So we also do this layer wise analyses and identify the difference. The latent codes have the different layers actually controlled difference, a level of concepts, questions. So how do you identify which unit does what and what is considered part of each unit? Yeah, that's also a very wonderful country. And so, so as an aside previously, we apply some of the shelf predictors to the output image. For this individual units, we also can apply some prediction models. So we apply some model called the semantic semantic segmentation model. So which can give a label for each one of the pixel for the generated image. Now we just calculates the correlation between the unit activation and the predicted semantic masks now associated label to each one of the units. Okay, so let's continue on. So it's a generative model, become a more and more powerful people starch generally model on all kinds of data. So here we have a generating model, people trained for cats generation. So here I show some images of cats or that synthesize the cats though, that pretty tunes and the Tao also looks very realistic. And also people came to some model, the generative model as a component on characters and can synthesize all kinds of Catalan characters. But for those kind of model is really difficult to control. Why? Because it's very difficult to get our predictors, you know, from our previous work, we have to have this off-the-shelf predictors to extract features from the US. This is how the image for those stalk and that any enemy images. So it's really hard to annotates those image. Therefore, we fertility brought some measure called the unsupervised learning of a steel rod dimensions, which means we would like to attend B5 is controllable on dimensions or without using labels. So it's more like a magic, but we still can do this. So what we are doing that here, we have a cat's generator. There are some technical here. So we are looking at this intermediate activations. So we actually found out the facial difference after this, the editing is independence inputs random vector. So therefore we develop some objective to maximize the operation of the difference. So this is unsupervised objective, so we can just optimize this. And then we had been Debye this steerable or diminishes. So students who are interested, you can take a look at papers. So here I just ignore the detail. So after we identify this steerable on dimensions, so we can have this very cool humanity low AAA account of creation. So this interface, so the sidebar on the left, control on different attributes correspond to a different app endophytes cerebral dimensions. Here I, let me just play this demo. So you can see here you can't just slide this bar. And now you can see it as the outputs image change accordingly. So we can like zoom-in, zoom-out for the cats and the change the pose of the cats. And also change that if firm, a style of this cannons color, firm, and texture of the image. So it's a very cool applications. It's also an interactive content creation. So I really believe this tide of humanity in the book and AI, Country creation will be up. We're providing direction for the future. You can imagine if a lot of industrial design or construction device, if you can train some generative model down, can't have human in the loop to interact with this AI model than we can better control the outputs. So that can greatly lower the barrier of consecration, maybe. But you can imagine even produce music composition. We can apply this kind of techniques for on music composition that will, that will greatly reduce the barrier of creation. Right? So any questions for this part? Otherwise, we'll move into the third part. Okay, so let's move into the third arch. So these parts we will look into how can we boots generalizable EI for this machine autonomy. So you know, nowadays, as AI become more and more powerful, people star on using this deep learning model for a lot of this on robotics, robotics or self-driving applications. So here we are looking at SIR model people use for the self-driving. So in that case, inputs could be different since are like backlight LIDAR sensor, RGB sensors. And so we input signals into the neural network. The neural network outputs the driving signal on how to steal the will, by the way, should accelerates and break. So to prototype this kind of deep learning models, so it's impossible to directly do this on the, on the roads, right? So we have to do this, the developments in, on simulation using. So one of the way, very popular approach for this machine autonomy is courts on reinforcement learning. So remote learning is a technique people use or for, for many embodied AI applications such as the RPA go and also this R4 start to play the game of Starcraft because of BPM, either use this, recoup money very widely on. So, so basically the cerebrum running the Dow granddad. So we have an agent parametrized by this neural network, so it will interact with the environment. So here we have a driving environment. So you will receive the rewards and obligations and the outputs. The action. However, for this title on LinkedIn Learning, so we change some age. So here we retrain a simulated environment. But we found out this agent is put each analyte, the board to some new environments. So here It's Trina age and just do a lot of crazy stuff inside the things just scratch other cars and hating to this practicum and caused a lot of trouble. So you can see also the costs also improved. So let's model have difficulties to generalize to new environments. Let's pick pose this kind of treaties and it's quite limited. So we only have a limited number of environments. And also the training process. It's showing the arrows to the safety of this refund is the US standing problems. So we have some recent works that try to improve the safety of the cerebrum and only agents. So what we are doing is that we puts a human in this loop in the self-learning MOOC. So instead of just interact with their environments. So we just put the experts inside this agent sites and try to interact with this learning process so you can match this. Does that how we human North, which Apple car. So I, I guess many of you have driver lessons. First to take the exam, right? Take that paper exam, then you will get that nonetheless permits. Then you can directly seats on the driver seats and a drop of carbs under the guidance of experts, all your parents just sitting on the passenger seat. Right. So in that case, so we can actually try to explore this rolling process under the guidance of some experts. So in this loop, we just put human in the loop in this learning process to ensure the safety? So here I just ignore all the mathematics. I just show a demo videos. So Gary, a student who might, so you can see gays are pretty happy, but he can play this video game or the time that generates a lot of demonstrations. So they just show this agents how to drive the cars. So when this car running into some risky behaviors on, so the students will intervene. This is running process and to try to demonstrate what is our correct way to do the behaviors. So here this video showing you this demonstration process. So we here we just speed up the learning process. So after about 20 minutes of demonstration. So you can see here that a student can't just a hands-off than this agent, can drive insights on this environments without much trouble up sometimes is you stop and you try to be more cautious about this obstacle course. So you can finish this navigations, only just tiny millions of demonstration. So that's tied up human in the loop. Learning is also very interesting. Directions. That's K, improve the sample efficiency. So here is some other work we have been doing that. So we also want to improve the diversity of environments. As I said, Why does the Asian pretty generalizable to new environment? Because the environment is not large enough. So in that case, we just set B phi some elementary and traffic blocks, such as this round about uncurved robe key intersections. Now after we have this element of the blocks, then we develop some algorithm that can just sample from that divides a black-box and gloom them scatter into some road network. They each road network can further turning into some training environments. Then we can generate lot of new environments than put our agents and for this training. And so here is our third row, the generation process. So by each generated no roads, we can change its roads into our training environments, can, can be interacted inactive or we die without agents. So here I show you some on diverse January day environments. So each January the environment can use AI into the training. And so after we have lot of generated environments and we just to show that's generalizability is graded in group. Hi, this environment biodiversity. So here the horizontal axis is the number of tree maps. On the vertical axis is just the performers. You can see after we have moduli maps than the test performers and the king performers, just a match with each other. So here is another demo video after this type of treaties. So we found out this agent can be quite amazing work. Right now the cost is pretty no and know how to even how to stop in front of some weighting night. And yeah, even know how to do some cake to find some on shortcut, like just how to drive through the chat to traffic home. So, so here we open source on our driving simulator. So you still ongoing, but the driving simulator already open source. So you can go there and get the Chrome and play with the simulators. Some students ask about how do you prevent overfitting in this scenario? I see how ground-based on one single student on drivings on yeah, that's also very interesting question. So here we just invites, saw in the demo video, I just show one student, but we might like 10 students. And the dry bound difference on, on roads have like 50 different roads. So student can just coming and drive, drive on different robes. Yes, children are actually pretty enjoying this process. Just more like playing some video games that we can collect. A lot of. This dam was human demonstrations that we can improve the performers on generalization ability of the agents. Okay, so we finished talking about all those three examples. So let's have a summary. So we would like to move towards humans. Ai, you know, lot of people in the fields be owning care about the performers. I think the performers of the AR model using beads, very important, but it shouldn't be the only metric we care about. We should also care about are many other attributes that's very relevant to our human beings. So as I showed you earlier, there are many other human centric of poverty we should also care about and also try to understand and improve, such as expandability, as durability and generalizability and modularity and down pounds of attributes or add a poverty's we should care about. And if we can improve those kind of properties, now we can make this AI more or runs and we can move more towards SQL Sentry. Also, there are many on impact areas are but human-centered AI, such as this, the content creation. So we have this interactive cointegration. Humor can work with the AR model to create new content. Also in those heels it here applications, you know, what Dr. want to see, not just the AR model. So the human doctor actually wanted to understand why this AI model give this a diagnosis, even a patient walk on stack, why this AI model see I have a cancer. You should show me the evidence and the reasons why. Also a lot of the physical system, such as self-driving. So we want this AI model, this AI car battery. In fact, it's a human driver, so we don't want to completely replace the human drivers. We just control the level of automation. So that will involve how, how can we balance under the control of the AI as well as the human driver? And also for this collaborative robots, we want so the robot can better understand the intent and the predict their behaviors. Also relevant some societal impacts, such as the Federalists and the bias of the AMO. Those when we deploy this model. We also want to build more interpretable models for this. Um, okay, so here is just a summary on so on. So we won't build more expandable syrup, I'm sure an eyespot, eventually more human-centric AIs. So all the papers and the codes are available as my webpage. So you can just Google me. So I wish you have a wonderful journey at UCLA. So I really look forward to meeting each of you in campus maybe early next year. Thank you. Thank you. Interesting directions. And hopefully some of the students in the growing listening will be for the few years. In this work, we have a minutes. So before mine, if anybody wants to ask a question in texting chat, actually go and see and asked the cushion normally. So if you if you have a question, raise your hand. I think they can. Yeah. You can probably just speak. Yeah. So anybody actually, I cannot. Okay. I cannot. Let me change the view to many people side-by-side gallery. Okay. So you've heard we have a u alone asked him a question. I haven't asked that question because I'm really interested in you said that the that the AIP may be creative. I was just wondering like Canada machine deep creative glide or was that like different type of view using a word like kinda lacZ gene by itself, just find something that's us, humans, I'm creating it. You can repeat the question. Yeah. We said that AI will be printed. Just wondering license, we are painting like these machines, the machines logical. I was just wondering, like chat and machines actually being creative work in New York minute. Yeah. So, yeah, that's a very good question. And so it just how you define creativity. Sometimes creativity could come in different ways. Could be it as compensation, channel creativity. Simple machines credit, pretty good that this kind of compensation or our creativity, know how to create some new concept by recompose some previous concepts. But if you see this creativity is more defy as like Craig knew, then that will be a still difficult for the machines. It just depends on which level you are talking about. Okay. Thank you. Absolutely. Yeah. I'm Brian. Okay. Okay. Though, I understand that that may be need AI you need are already capable of generating random image, get paid on time parameter. But how, how, how about our, our, our dough, my comp by generating random big deal because video pick a multiple brain and could perhaps be determined by an algorithm in the future. Yeah, that's a good question. So right now the modal generative models do you add to the stage of image generation? So video generation is still an unsolved problem. Why? It's still ongoing? So it's actually a hot topic in the research of computer vision. So people try to develop, but it's a temporal or sequential model. Try to generate like frame by frame, so that, so it's an unsolved Poverty yet. Okay, and let's take it in the last lesson. Lesson with sharing is based on. So I think what you're talking about unsupervised syllable models and where you're able to generate like what features are able to be steered. I don't think I saw like white human understandable names for each of those features. Is that something that can be done with AI or is or is assuming necessary to do that. Yes, thanks for the question. So that's a wonderful question. So so right now we aren't doing this unsupervised attribute discovery. So a lot of the attributes, I think half of the attributes on our live with human concept and another half, like if people don't have a concretes name of four, this is actually the beauty of this work. It's more like sometimes when a machine can help us to identify this control both dimensions. Like even we don't have a name for this. But the one when we just draw this bar and then people say, oh, this, this is still a meaningful directions. So how to better annotates those are discovered dimension is also an unsolved problem. So we try to use this is crowdsourcing platform to annotates the CSF around the diminishes. But sometimes this dimension are not perfectly our lights. So we don't have a name for this. The thing about half of them have this very clearly. Chemo can just write some sentence to describe what it is he mentioned was about. Okay. Thank you. Can you give us one more quick question more? I think with the avocado chair, is that a combination of using natural language processing to transfer to like understand the sentence being typed in and then feeding that into like a steerable. Again. Yeah, that's more like the Open AI model caudally. So you can actually Google with this dilemma model. So they actually take the C inputs, nature and angry sentence. And it's more like a, people can use this nation language sentence to control this generation posted. But I think there are some space to combine the two to have more like fine-grained control on the synthesized image. But unfortunately they don't release that's pre-trained model yet, but I guess they just take too much to tuner models. The OpenAI, the boundary is much Model. So people actually waiting for this model to be released and we can play with them. When you think, okay, let me just finish my question. My question about the statement. I know I think I know the answer, which not much has been done. Issues to think about the self-driving, but we are seeing teaching the, the counter divided by human lightning is reinforcement learning. We know that actually if you don't have too much on the environment, was changing. For example, machines using mathematics, using control, control theory. If you give it to, for example, the dogfight between fighter jets, machines will perform better than human doing mathematical calculation. So I think this is a, an interesting area of research I do about somebody stands for GE actually doing better at doing this, which is a combine mathematics and deep learning. And that's do you did you do anything in this direction? Yeah. Yeah. Yeah. I think that's a wonderful directions. And Monica, like a mathematics, you can think about this as a more symbolic description for this process. So you can describe the deploring with some mathematical expressions and you actually can make this control easier. So this kind of this hybrid model, I think it's about the hydrogens. Yeah. How can we combine deep learning with a symbolic expressions on mathematic expression, even more exotic cars on causal inference. I cut down by Judea or Peru. That will be allowed under 40 reaction. Okay, For which is we are seeing the emergence of Brave New World. We choose which will be your world. So thank you all for coming to this class I gave you, but I didn't look here to tell you about next week. But I hope you enjoyed this is the way that it will continue. And now the TA and myself have to now to think about how do we give you some homework about the solids? This is Zoe, I will send you a link and we will talk care, Let's finish it now about what we do in the recitation. Thank you again bullae. And see you in the corner. You know, you are not heal this quarter, but we'll get compensation in the winter. You thank you. All bye. All right. Buh-bye. Buh-bye.